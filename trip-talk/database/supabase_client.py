import os
import json
from dotenv import load_dotenv
from supabase import create_client, Client
from langchain_community.vectorstores import SupabaseVectorStore
from langchain_openai import OpenAIEmbeddings
from langchain_core.documents import Document

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

class GuideCache:
    def __init__(self):
        self.supabase_url = os.environ.get("SUPABASE_URL")
        self.supabase_key = os.environ.get("SUPABASE_KEY")
        self.enabled = bool(self.supabase_url and self.supabase_key)
        
        if self.enabled:
            print("âœ… Supabase Cache Enabled")
            self.client: Client = create_client(self.supabase_url, self.supabase_key)
            self.embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
            
            # í…Œì´ë¸” ì´ë¦„ì´ 'documents'ì´ê³  query_nameì´ 'match_documents'ì¸ ê²ƒìœ¼ë¡œ ê°€ì • (LangChain ê¸°ë³¸ê°’)
            # ì‚¬ìš©ìê°€ Supabase SQL Editorì—ì„œ í•´ë‹¹ í…Œì´ë¸”ê³¼ í•¨ìˆ˜ë¥¼ ìƒì„±í•´ì•¼ í•¨.
            self.vector_store = SupabaseVectorStore(
                client=self.client,
                embedding=self.embeddings,
                table_name="documents",
                query_name="match_documents"
            )
        else:
            print("âš ï¸ Supabase Credentials missing. Caching is DISABLED.")

    async def search_guide(self, location: str, situation: str, threshold: float = 0.78):
        """
        ì£¼ì–´ì§„ ì¥ì†Œì™€ ìƒí™©ì— ëŒ€í•œ ê°€ì´ë“œê°€ ìºì‹œì— ìˆëŠ”ì§€ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        ìœ ì‚¬ë„ê°€ threshold ì´ìƒì¸ ê²½ìš° ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        (0.9 -> 0.78 ë¡œ ì™„í™”: 'ì˜¤ì‚¬ì¹´ ë¼ë©˜' vs 'ì˜¤ì‚¬ì¹´ ë¼ë©´' ì •ë„ì˜ ì°¨ì´ë¥¼ í—ˆìš©í•˜ê¸° ìœ„í•¨)
        """
        if not self.enabled:
            return None
            
        query_text = f"Location: {location}, Situation: {situation}"
        print(f"ğŸ” Searching cache for: {query_text}...")
        
        try:
            # LangChainì˜ similarity_search_with_relevance_scores ì‚¬ìš©
            # Note: SupabaseVectorStore implementation might vary, ensuring synchronous call works or wrapping it if needed.
            # Most vector stores in LangChain are synchronous. 
            # We run this potentially blocking call. In a full async app, we might want run_in_executor.
            
            # LangChain ëŒ€ì‹  ì§ì ‘ RPC í˜¸ì¶œ (í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°)
            query_embedding = self.embeddings.embed_query(query_text)
            
            params = {
                "query_embedding": query_embedding,
                "match_threshold": threshold, # 0.78 etc.
                "match_count": 1
            }
            
            # ì§ì ‘ RPC í˜¸ì¶œ
            response = self.client.rpc("match_documents", params).execute()
            
            # Supabase Python v2+ response format: response.data
            results = response.data
            
            if not results:
                print("Cache Miss (No results)")
                return None
                
            # ê²°ê³¼: [{'id':..., 'content':..., 'metadata':..., 'similarity':...}]
            best_match = results[0]
            score = best_match.get("similarity", 0)
            print(f"Cache Score: {score}")
            
            if score >= threshold:
                print("âš¡ Cache HIT!")
                return best_match.get("metadata", {}).get("guide_json")
            else:
                print("Cache Miss (Low similarity)")
                return None
                
        except Exception as e:
            print(f"Cache Search Error: {e}")
            return None

    async def save_guide(self, location: str, situation: str, guide_data: dict):
        """
        ìƒì„±ëœ ê°€ì´ë“œë¥¼ Supabaseì— ì €ì¥í•©ë‹ˆë‹¤.
        """
        if not self.enabled:
            return
            
        text_content = f"Location: {location}, Situation: {situation}"
        embedding = self.embeddings.embed_query(text_content)
        
        metadata = {
            "guide_json": guide_data,
            "location": location,
            "situation": situation
        }
        
        row = {
            "content": text_content,
            "metadata": metadata,
            "embedding": embedding
        }
        
        try:
            print("ğŸ’¾ Saving to cache...")
            # ì§ì ‘ Insert í˜¸ì¶œ
            self.client.table("documents").insert(row).execute()
            print("âœ… Saved to cache.")
        except Exception as e:
            print(f"Cache Save Error: {e}")
