{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ec2e9aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ebc093c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastapi\n",
      "  Downloading fastapi-0.128.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting starlette<0.51.0,>=0.40.0 (from fastapi)\n",
      "  Downloading starlette-0.50.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: pydantic>=2.7.0 in c:\\users\\asguug\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from fastapi) (2.12.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\asguug\\appdata\\roaming\\python\\python311\\site-packages (from fastapi) (4.15.0)\n",
      "Collecting annotated-doc>=0.0.2 (from fastapi)\n",
      "  Downloading annotated_doc-0.0.4-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\asguug\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from pydantic>=2.7.0->fastapi) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\asguug\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from pydantic>=2.7.0->fastapi) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\asguug\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from pydantic>=2.7.0->fastapi) (0.4.2)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in c:\\users\\asguug\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from starlette<0.51.0,>=0.40.0->fastapi) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\asguug\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<0.51.0,>=0.40.0->fastapi) (3.11)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\asguug\\.pyenv\\pyenv-win\\versions\\3.11.9\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<0.51.0,>=0.40.0->fastapi) (1.3.1)\n",
      "Downloading fastapi-0.128.0-py3-none-any.whl (103 kB)\n",
      "   ---------------------------------------- 0.0/103.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 103.1/103.1 kB 5.8 MB/s eta 0:00:00\n",
      "Downloading annotated_doc-0.0.4-py3-none-any.whl (5.3 kB)\n",
      "Downloading starlette-0.50.0-py3-none-any.whl (74 kB)\n",
      "   ---------------------------------------- 0.0/74.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 74.0/74.0 kB 4.0 MB/s eta 0:00:00\n",
      "Installing collected packages: annotated-doc, starlette, fastapi\n",
      "Successfully installed annotated-doc-0.0.4 fastapi-0.128.0 starlette-0.50.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script fastapi.exe is installed in 'c:\\Users\\asguug\\.pyenv\\pyenv-win\\versions\\3.11.9\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: c:\\Users\\asguug\\.pyenv\\pyenv-win\\versions\\3.11.9\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install fastapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc079ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.runnables import chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate ,MessagesPlaceholder\n",
    "from langchain_classic.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53f086e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asguug\\AppData\\Local\\Temp\\ipykernel_119372\\1427356914.py:18: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the `langchain-tavily package and should be used instead. To use it run `pip install -U `langchain-tavily` and import as `from `langchain_tavily import TavilySearch``.\n",
      "  search_tool = TavilySearchResults(k=3)\n"
     ]
    }
   ],
   "source": [
    "#8ì£¼ì°¨ ì„œë¸Œê·¸ë˜í”„ + êµ¬ì¡°í™”ëœ ì¶œë ¥ê³¼ ì•ˆì „í•œ ì œì–´\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "from typing import Annotated, List, TypedDict, Optional, Dict, Any, Literal\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "search_tool = TavilySearchResults(k=3)\n",
    "\n",
    "    \n",
    "class ResearchState(TypedDict):\n",
    "    topic:str\n",
    "    logs:Annotated[List[BaseMessage] ,add_messages]\n",
    "    raw_data:str\n",
    "    quality:str\n",
    "    retry_count:int\n",
    "\n",
    "def research_execute_node(state: ResearchState):\n",
    "    print(\"\\[Research] ì •ë³´ ìˆ˜ì§‘ ì¤‘...\")\n",
    "    topic = state[\"topic\"]\n",
    "    \n",
    "    # ì‹¤ì œ ê²€ìƒ‰ ìˆ˜í–‰\n",
    "    try:\n",
    "        results = search_tool.invoke(topic)\n",
    "        content = \"\\n\".join([r[\"content\"] for r in results])\n",
    "    except:\n",
    "        content = \"ê²€ìƒ‰ ì‹¤íŒ¨\"\n",
    "        \n",
    "    return {\n",
    "        \"raw_data\": content, \n",
    "        \"logs\": [AIMessage(content=f\"ê²€ìƒ‰ ì™„ë£Œ: {len(content)}ì\", name=\"researcher\")]\n",
    "    }\n",
    "    \n",
    "def research_reflect_node(state: ResearchState):\n",
    "    print(\"[Research Sub] ì •ë³´ ì¶©ë¶„ì„± í‰ê°€ ì¤‘...\")\n",
    "    \n",
    "    # LLMì´ ì§ì ‘ í‰ê°€\n",
    "    chain = ChatPromptTemplate.from_template(\n",
    "        \"\"\"ë‹¹ì‹ ì€ ì—„ê²©í•œ ì—°êµ¬ íŒ€ì¥ì…ë‹ˆë‹¤. ìˆ˜ì§‘ëœ ìë£Œê°€ ì£¼ì œ '{topic}'ì„ ì„¤ëª…í•˜ê¸°ì— ì¶©ë¶„í•œì§€ í‰ê°€í•˜ì„¸ìš”.\n",
    "        \n",
    "        [ìˆ˜ì§‘ëœ ìë£Œ]\n",
    "        {data}\n",
    "        \n",
    "        ìë£Œê°€ ì£¼ì œë¥¼ í¬ê´„ì ìœ¼ë¡œ ì„¤ëª…í•˜ë©´ 'PASS', ë¶€ì¡±í•˜ê±°ë‚˜ í¸í–¥ë˜ì—ˆë‹¤ë©´ 'FAIL'ì´ë¼ê³ ë§Œ ë‹µí•˜ì„¸ìš”.\n",
    "        \"\"\"\n",
    "    ) | llm | StrOutputParser()\n",
    "    \n",
    "    evaluation = chain.invoke({\"topic\": state[\"topic\"], \"data\": state[\"raw_data\"]})\n",
    "    quality = \"PASS\" if \"PASS\" in evaluation else \"FAIL\"\n",
    "    \n",
    "    print(f\"      ã„´ í‰ê°€ ê²°ê³¼: {quality}\")\n",
    "    return {\"quality\": quality, \"logs\": [AIMessage(content=f\"í‰ê°€ ê²°ê³¼: {quality}\", name=\"evaluator\")]}\n",
    "\n",
    "def research_revise_node(state: ResearchState):\n",
    "    print(\" [Research] ì¶”ê°€ ê²€ìƒ‰(ë³´ì™„) ìˆ˜í–‰ ì¤‘...\")\n",
    "    \n",
    "    topic = state[\"topic\"]\n",
    "    current_data = state[\"raw_data\"]\n",
    "    \n",
    "    # 1. LLMì—ê²Œ \"ë¶€ì¡±í•œ ë‚´ìš©ì„ ì°¾ê¸° ìœ„í•œ ê²€ìƒ‰ì–´\"ë¥¼ ìƒì„±í•˜ê²Œ ì‹œí‚´\n",
    "    query_chain = ChatPromptTemplate.from_template(\n",
    "        \"\"\"ë‹¹ì‹ ì€ ë…¸ë ¨í•œ ë¦¬ì„œì²˜ì…ë‹ˆë‹¤.\n",
    "        ì£¼ì œ '{topic}'ì— ëŒ€í•´ í˜„ì¬ ìˆ˜ì§‘ëœ ìë£Œê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
    "        \n",
    "        [í˜„ì¬ ìë£Œ]\n",
    "        {data}\n",
    "        \n",
    "        ìœ„ ìë£Œì—ì„œ ë¹ ì§„ ë‚´ìš©ì´ë‚˜ ë” êµ¬ì²´ì ì¸ ì •ë³´ê°€ í•„ìš”í•œ ë¶€ë¶„ì„ íŒŒì•…í•˜ì—¬,\n",
    "        ê²€ìƒ‰ ì—”ì§„ì— ì…ë ¥í•  'êµ¬ì²´ì ì¸ ì¶”ê°€ ê²€ìƒ‰ì–´' 1ê°œë¥¼ ì œì•ˆí•´ì£¼ì„¸ìš”. (ì„¤ëª… ì—†ì´ ê²€ìƒ‰ì–´ë§Œ ì¶œë ¥)\n",
    "        \"\"\"\n",
    "    ) | llm | StrOutputParser()\n",
    "    \n",
    "    # ë°ì´í„°ê°€ ë„ˆë¬´ ê¸¸ë©´ í† í° ë¹„ìš©ì´ ë“œë‹ˆ ë’¤ìª½ ì¼ë¶€ë§Œ ì°¸ê³ í•˜ê±°ë‚˜ ìš”ì•½í•´ì„œ ë„£ì„ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n",
    "    # ì—¬ê¸°ì„œëŠ” ì•ë¶€ë¶„ 2000ìë§Œ ì°¸ê³ í•˜ë„ë¡ ì œí•œí•˜ê² ìŠµë‹ˆë‹¤.\n",
    "    new_query = query_chain.invoke({\"topic\": topic, \"data\": current_data[:2000]})\n",
    "    print(f\"      ã„´ìƒì„±ëœ ì¶”ê°€ ê²€ìƒ‰ì–´: '{new_query}'\")\n",
    "    \n",
    "    # 2. ìƒì„±ëœ ê²€ìƒ‰ì–´ë¡œ ì‹¤ì œ ê²€ìƒ‰ ìˆ˜í–‰\n",
    "    try:\n",
    "        search_results = search_tool.invoke(new_query)\n",
    "        new_content = \"\\n\".join([f\"- {r['content']}\" for r in search_results])\n",
    "    except Exception as e:\n",
    "        new_content = f\"ì¶”ê°€ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}\"\n",
    "        \n",
    "    # 3. ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•© (Merge)\n",
    "    # ê¸°ì¡´ ë°ì´í„° + [ì¶”ê°€ ë³´ì™„ ë°ì´í„°] í˜•ì‹ìœ¼ë¡œ í•©ì¹¨\n",
    "    combined_data = current_data + f\"\\n\\n[ì¶”ê°€ ê²€ìƒ‰ ê²°ê³¼ ({new_query})]:\\n\" + new_content\n",
    "    \n",
    "    return {\n",
    "        \"raw_data\": combined_data, \n",
    "        \"retry_count\": state.get(\"retry_count\", 0) + 1,\n",
    "        \"logs\": [AIMessage(content=f\"ì¶”ê°€ ê²€ìƒ‰ ì™„ë£Œ: {new_query}\", name=\"researcher\")]\n",
    "    }\n",
    "    \n",
    "def research_submit_node(state: ResearchState):\n",
    "    # ìµœì¢… ê²°ê³¼ ì •ë¦¬\n",
    "    summary_chain = ChatPromptTemplate.from_template(\n",
    "        \"ë‹¤ìŒ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ '{topic}'ì— ëŒ€í•œ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½ ì •ë¦¬í•´ì¤˜:\\n\\n{data}\"\n",
    "    ) | llm | StrOutputParser()\n",
    "    final_summary = summary_chain.invoke({\"topic\": state[\"topic\"], \"data\": state[\"raw_data\"]})\n",
    "    if \"run_id\" in state:\n",
    "        save_step_to_file(state[\"run_id\"], \"Research_Done\", {\"summary\": final_summary})\n",
    "        \n",
    "    return {\"raw_data\": final_summary}\n",
    "\n",
    "research_workflow = StateGraph(ResearchState)\n",
    "research_workflow.add_node(\"execute\", research_execute_node)\n",
    "research_workflow.add_node(\"reflect\", research_reflect_node)\n",
    "research_workflow.add_node(\"revise\", research_revise_node)\n",
    "research_workflow.add_node(\"submit\", research_submit_node)\n",
    "\n",
    "research_workflow.add_edge(START, \"execute\")\n",
    "research_workflow.add_edge(\"execute\", \"reflect\")\n",
    "\n",
    "def route_research(state: ResearchState):\n",
    "    # FAILì´ê³  ì¬ì‹œë„ 1íšŒ ë¯¸ë§Œì´ë©´ ë³´ì™„(revise), ì•„ë‹ˆë©´ ì œì¶œ(submit)\n",
    "    if state[\"quality\"] == \"FAIL\" and state.get(\"retry_count\", 0) < 1:\n",
    "        return \"revise\"\n",
    "    return \"submit\"\n",
    "\n",
    "research_workflow.add_conditional_edges(\"reflect\", route_research, {\"submit\": \"submit\", \"revise\": \"revise\"})\n",
    "research_workflow.add_edge(\"revise\", \"submit\")\n",
    "research_workflow.add_edge(\"submit\", END)\n",
    "\n",
    "research_app = research_workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55d84248",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WriterState(TypedDict):\n",
    "    topic: str\n",
    "    research_data: str\n",
    "    draft: str\n",
    "    critique: str\n",
    "    score: float\n",
    "    revision_count: int\n",
    "    logs: Annotated[List[BaseMessage], add_messages]\n",
    "    \n",
    "def writer_execute_node(state: WriterState):\n",
    "    count = state.get('revision_count', 0)\n",
    "    print(f\"[Writer Sub] ê¸€ ì‘ì„± ì¤‘... (ë²„ì „ {count + 1})\")\n",
    "    \n",
    "    # ì‹¤ì œ ê¸€ì“°ê¸° ìˆ˜í–‰\n",
    "    chain = ChatPromptTemplate.from_template(\n",
    "        \"\"\"ë‹¹ì‹ ì€ ìƒí™©ì— ë§ì¶° ìµœì ì˜ ê¸€ì„ ì“°ëŠ” 'ì „ë¬¸ ìˆ˜ì„ ì—ë””í„°'ì…ë‹ˆë‹¤.\n",
    "        ì œê³µëœ ì¬ë£Œë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì£¼ì œ '{topic}'ì— ê°€ì¥ ì í•©í•œ í˜•ì‹ì˜ ë¬¸ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.\n",
    "        \n",
    "        [ì…ë ¥ ìë£Œ]\n",
    "        1. ì—°êµ¬ ë‚´ìš©: {data}\n",
    "        2. ì½”ë“œ ì˜ˆì œ: {code} (ì—†ìœ¼ë©´ 'ì—†ìŒ')\n",
    "        3. êµ¬ì¡°ë„(Mermaid): {design} (ì—†ìœ¼ë©´ 'ì—†ìŒ')\n",
    "        4. ì´ì „ ë¹„í‰: {critique}\n",
    "        \n",
    "        [ì‘ì„± ì§€ì¹¨]\n",
    "        1. í˜•ì‹ íŒë‹¨: \n",
    "           - ì½”ë“œ/êµ¬ì¡°ë„ê°€ ìˆë‹¤ë©´ 'ê¸°ìˆ  ë¬¸ì„œ'ë‚˜ 'íŠœí† ë¦¬ì–¼' í˜•ì‹ìœ¼ë¡œ, \n",
    "           - ì—†ë‹¤ë©´ 'ì—ì„¸ì´', 'ê¸°íšì„œ', 'ë³´ê³ ì„œ' ë“± ì£¼ì œì— ë§ëŠ” í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n",
    "           \n",
    "        2. ìë£Œ í†µí•© (ì¡°ê±´ë¶€ ì‚½ì…):\n",
    "           - ì—°êµ¬ ë‚´ìš©: ê¸€ì˜ ë…¼ë¦¬ì  ê·¼ê±°ë¡œ í™œìš©í•˜ì„¸ìš”.\n",
    "           - ì½”ë“œ ì˜ˆì œ: ë‚´ìš©ì´ 'ì—†ìŒ'ì´ ì•„ë‹ˆë¼ë©´, ë°˜ë“œì‹œ ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡(```python ... ```)**ìœ¼ë¡œ ë³¸ë¬¸ì˜ ì ì ˆí•œ ìœ„ì¹˜ì— ì‚½ì…í•˜ì„¸ìš”. (ì–µì§€ë¡œ ë§Œë“¤ì§€ ë§ˆì„¸ìš”)\n",
    "           - êµ¬ì¡°ë„: ë‚´ìš©ì´ 'ì—†ìŒ'ì´ ì•„ë‹ˆë¼ë©´, ë°˜ë“œì‹œ Mermaid ì½”ë“œ ë¸”ë¡(```mermaid ... ```)**ìœ¼ë¡œ ì‹œê°í™” ì„¹ì…˜ì— ì‚½ì…í•˜ì„¸ìš”.\n",
    "           \n",
    "        3. ìŠ¤íƒ€ì¼:\n",
    "           - ì£¼ì œê°€ í•™ìˆ ì ì´ë©´ ì „ë¬¸ì ìœ¼ë¡œ, ëŒ€ì¤‘ì ì´ë©´ ì½ê¸° ì‰½ê²Œ ì‘ì„±í•˜ì„¸ìš”.\n",
    "           - ì„œë¡ -ë³¸ë¡ -ê²°ë¡ ì˜ ì™„ê²°ì„± ìˆëŠ” êµ¬ì¡°ë¥¼ ê°–ì¶”ì„¸ìš”.\n",
    "        \"\"\"\n",
    "    ) | llm | StrOutputParser()\n",
    "    \n",
    "    draft = chain.invoke({\n",
    "        \"topic\": state[\"topic\"],\n",
    "        \"data\": state.get(\"research_data\", \"ìë£Œ ì—†ìŒ\"),\n",
    "        \"code\": state.get(\"code_data\", \"ì—†ìŒ\"), \n",
    "        \"design\": state.get(\"design_data\", \"ì—†ìŒ\"), \n",
    "        \"critique\": state.get(\"critique\", \"ì—†ìŒ\")\n",
    "    })\n",
    "    if \"run_id\" in state:\n",
    "        save_step_to_file(state[\"run_id\"], \"Write_Done\", {\"summary\": draft})\n",
    "    return {\n",
    "        \"draft\": draft, \n",
    "        \"revision_count\": count + 1,\n",
    "        \"logs\": [AIMessage(content=f\"ì´ˆì•ˆ v{count+1} ì‘ì„± ì™„ë£Œ\", name=\"writer\")]\n",
    "    }\n",
    "\n",
    "def writer_reflect_node(state: WriterState):\n",
    "    print(\"[Writer Sub] í’ˆì§ˆ í‰ê°€ ì¤‘...\")\n",
    "    \n",
    "    # ì‹¤ì œ í‰ê°€ ìˆ˜í–‰\n",
    "    chain = ChatPromptTemplate.from_template(\n",
    "        \"\"\"ë‹¹ì‹ ì€ ì„¸ê³„ì ì¸ ì €ë„ì˜ 'ì—„ê²©í•œ ìˆ˜ì„ í¸ì§‘ì'ì…ë‹ˆë‹¤. \n",
    "        ì•„ë˜ ê¸€ì´ ì‚¬ìš©ì ìš”ì²­ ì£¼ì œì¸ '{topic}'ì— ì™„ë²½í•˜ê²Œ ë¶€í•©í•˜ëŠ”ì§€ ë¹„íŒì ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.\n",
    "        \n",
    "        [í‰ê°€ ê¸°ì¤€]\n",
    "        1. ì£¼ì œ ì í•©ì„±: ìš”ì²­í•œ ì£¼ì œë¥¼ ì •í™•íˆ ë‹¤ë£¨ê³  ìˆëŠ”ê°€?\n",
    "        2. êµ¬ì²´ì„±: ë§‰ì—°í•œ ë‚´ìš©ì´ ì•„ë‹ˆë¼ êµ¬ì²´ì ì¸ ì‚¬ì‹¤/ì˜ˆì‹œê°€ ìˆëŠ”ê°€?\n",
    "        3. ë…¼ë¦¬ì  íë¦„: ì„œë¡ -ë³¸ë¡ -ê²°ë¡ ì˜ êµ¬ì¡°ê°€ íƒ„íƒ„í•œê°€?\n",
    "        \n",
    "        ì£¼ì˜: ì¡°ê¸ˆì´ë¼ë„ ëª¨í˜¸í•˜ê±°ë‚˜, í‰ë²”í•œ ë‚´ìš©ì´ë¼ë©´ 7ì  ë¯¸ë§Œìœ¼ë¡œ ì ìˆ˜ë¥¼ ì£¼ì„¸ìš”. \n",
    "        ì™„ë²½í•˜ì§€ ì•Šìœ¼ë©´ 9ì  ì´ìƒì„ ì£¼ì§€ ë§ˆì„¸ìš”.\n",
    "        \n",
    "        í˜•ì‹: ì ìˆ˜/êµ¬ì²´ì ì¸_í”¼ë“œë°± (ì˜ˆ: 6.5/ì£¼ì œì™€ ê´€ë ¨ ì—†ëŠ” ë‚´ìš©ì´ í¬í•¨ë˜ì–´ ìˆê³  ì˜ˆì‹œê°€ ë¶€ì¡±í•©ë‹ˆë‹¤)\n",
    "        \n",
    "        [ê¸€]: {draft}\n",
    "        \"\"\"\n",
    "    ) | llm | StrOutputParser()\n",
    "    \n",
    "    response = chain.invoke({\n",
    "        \"draft\": state[\"draft\"],\n",
    "        \"topic\": state[\"topic\"] \n",
    "    })\n",
    "    \n",
    "    try:\n",
    "        score_str, fb = response.split(\"/\", 1)\n",
    "        score = float(score_str.strip().replace(\"ì \", \"\"))\n",
    "    except:\n",
    "        score, fb = 5.0, \"í˜•ì‹ ì˜¤ë¥˜\"\n",
    "        \n",
    "    print(f\"      ã„´ ì ìˆ˜: {score}ì \")\n",
    "    print(f\"      ã„´  í”¼ë“œë°±: {fb[:100]}...\")\n",
    "    \n",
    "    return {\n",
    "        \"score\": score, \n",
    "        \"critique\": fb,\n",
    "        \"logs\": [AIMessage(content=f\"í‰ê°€: {score}ì  / {fb}\", name=\"critic\")]\n",
    "    }\n",
    "    \n",
    "writer_workflow = StateGraph(WriterState)\n",
    "writer_workflow.add_node(\"execute\", writer_execute_node)\n",
    "writer_workflow.add_node(\"reflect\", writer_reflect_node)\n",
    "\n",
    "writer_workflow.add_edge(START, \"execute\")\n",
    "writer_workflow.add_edge(\"execute\", \"reflect\")\n",
    "\n",
    "def route_writer(state: WriterState):\n",
    "    # 8.5ì  ì´ìƒì´ê±°ë‚˜ 3ë²ˆ ìˆ˜ì •í–ˆìœ¼ë©´ ì¢…ë£Œ\n",
    "    if state[\"score\"] >= 8.5 or state[\"revision_count\"] >= 3:\n",
    "        return \"end\"\n",
    "    return \"execute\" # ë‹¤ì‹œ ì‘ì„±\n",
    "\n",
    "writer_workflow.add_conditional_edges(\"reflect\", route_writer, {\"execute\": \"execute\", \"end\": END})\n",
    "writer_app = writer_workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94ece4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeState(TypedDict):\n",
    "    topic: str\n",
    "    logs: Annotated[List[BaseMessage], add_messages]\n",
    "    code_result: str\n",
    "    critique: str   # ë¦¬ë·° í”¼ë“œë°± ì €ì¥\n",
    "    quality: str    # PASS / FAIL\n",
    "    retry_count: int\n",
    "\n",
    "def code_execute_node(state: CodeState):\n",
    "    print(f\"[Code Agent] '{state['topic']}' ì½”ë“œ ì´ˆì•ˆ ì‘ì„± ì¤‘...\")\n",
    "    \n",
    "    # ì´ˆì•ˆ ì‘ì„± í”„ë¡¬í”„íŠ¸\n",
    "    chain = ChatPromptTemplate.from_template(\n",
    "        \"\"\"ë‹¹ì‹ ì€ Senior Python ê°œë°œìì…ë‹ˆë‹¤. \n",
    "        ì£¼ì œ '{topic}'ì— ëŒ€í•œ Python ì˜ˆì œ ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”.\n",
    "        \n",
    "        [ìš”êµ¬ì‚¬í•­]\n",
    "        1. ì‹¤í–‰ ê°€ëŠ¥í•œ Python ì½”ë“œì—¬ì•¼ í•©ë‹ˆë‹¤.\n",
    "        2. ì½”ë“œ ë‚´ì— ìƒì„¸í•œ ì£¼ì„(Comments)ì„ í¬í•¨í•˜ì„¸ìš”.\n",
    "        3. ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡(```python ... ```)ìœ¼ë¡œ ê°ì‹¸ì§€ ë§ê³  ìˆœìˆ˜ ì½”ë“œë§Œ ì¶œë ¥í•˜ê±°ë‚˜, \n",
    "           ì½”ë“œ ë¸”ë¡ì„ ì“´ë‹¤ë©´ íŒŒì‹± ê°€ëŠ¥í•œ í˜•íƒœë¡œ ì£¼ì„¸ìš”.\n",
    "        \"\"\"\n",
    "    ) | llm | StrOutputParser()\n",
    "    \n",
    "    code = chain.invoke({\"topic\": state[\"topic\"]})\n",
    "    \n",
    "    if \"run_id\" in state:\n",
    "        save_step_to_file(state[\"run_id\"], \"Code_Done\", {\"summary\": code})\n",
    "        \n",
    "    return {\n",
    "        \"code_result\": code, \n",
    "        \"retry_count\": 0,\n",
    "        \"logs\": [AIMessage(content=\"ì½”ë“œ ì´ˆì•ˆ ìƒì„± ì™„ë£Œ\", name=\"coder\")]\n",
    "    }\n",
    "    \n",
    "def code_reflect_node(state: CodeState):\n",
    "    print(\"[Code Agent] ì½”ë“œ í’ˆì§ˆ ë¦¬ë·° ì¤‘...\")\n",
    "    \n",
    "    # ë¦¬ë·° í”„ë¡¬í”„íŠ¸: ì—„ê²©í•˜ê²Œ í‰ê°€\n",
    "    chain = ChatPromptTemplate.from_template(\n",
    "        \"\"\"ë‹¹ì‹ ì€ ê¹Œë‹¤ë¡œìš´ ì½”ë“œ ë¦¬ë·°ì–´(Code Reviewer)ì…ë‹ˆë‹¤.\n",
    "        ì•„ë˜ ì½”ë“œë¥¼ ê²€í† í•˜ê³  ì ìˆ˜ì™€ í”¼ë“œë°±ì„ ì œê³µí•˜ì„¸ìš”.\n",
    "        \n",
    "        [ê²€í† í•  ì½”ë“œ]\n",
    "        {code}\n",
    "        \n",
    "        [í‰ê°€ ê¸°ì¤€]\n",
    "        1. ë¬¸ë²• ì˜¤ë¥˜(Syntax Error)ê°€ ì—†ëŠ”ê°€?\n",
    "        2. ì£¼ì„(Comments)ì´ ì¶©ë¶„íˆ ì‘ì„±ë˜ì—ˆëŠ”ê°€?\n",
    "        3. ì‹¤í–‰ ê°€ëŠ¥í•œ êµ¬ì¡°ì¸ê°€?\n",
    "        \n",
    "        [ì¶œë ¥ í˜•ì‹]\n",
    "        ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”:\n",
    "        ìƒíƒœ: [PASS ë˜ëŠ” FAIL]\n",
    "        í”¼ë“œë°±: [êµ¬ì²´ì ì¸ ê°œì„ ì  ë˜ëŠ” ì˜¤ë¥˜ ë‚´ìš©]\n",
    "        \"\"\"\n",
    "    ) | llm | StrOutputParser()\n",
    "    \n",
    "    review_result = chain.invoke({\"code\": state[\"code_result\"]})\n",
    "    \n",
    "    # ê²°ê³¼ íŒŒì‹± (PASS/FAIL ë¶„ë¦¬)\n",
    "    try:\n",
    "        status_line = review_result.split(\"\\n\")[0] # ì²« ì¤„ \"ìƒíƒœ: PASS\" ë“±\n",
    "        quality = \"PASS\" if \"PASS\" in status_line else \"FAIL\"\n",
    "        critique = review_result\n",
    "    except:\n",
    "        quality = \"FAIL\"\n",
    "        critique = \"ë¦¬ë·° í˜•ì‹ ì˜¤ë¥˜ ë°œìƒ\"\n",
    "\n",
    "    print(f\"      ã„´ ë¦¬ë·° ê²°ê³¼: {quality}\")\n",
    "    if quality == \"FAIL\":\n",
    "         print(f\"      ã„´í”¼ë“œë°±: {critique.split('í”¼ë“œë°±:')[1][:50]}...\") # ë¡œê·¸ ê°„ì†Œí™”\n",
    "         \n",
    "    return {\n",
    "        \"quality\": quality, \n",
    "        \"critique\": critique,\n",
    "        \"logs\": [AIMessage(content=f\"ë¦¬ë·° ì™„ë£Œ: {quality}\", name=\"reviewer\")]\n",
    "    }\n",
    "    \n",
    "def code_revise_node(state: CodeState):\n",
    "    print(\" [Code Agent] í”¼ë“œë°± ë°˜ì˜í•˜ì—¬ ì½”ë“œ ìˆ˜ì • ì¤‘...\")\n",
    "    \n",
    "    # ìˆ˜ì • í”„ë¡¬í”„íŠ¸\n",
    "    chain = ChatPromptTemplate.from_template(\n",
    "        \"\"\"ë‹¹ì‹ ì€ ê°œë°œìì…ë‹ˆë‹¤. ë¦¬ë·°ì–´ì˜ í”¼ë“œë°±ì„ ë°˜ì˜í•˜ì—¬ ì½”ë“œë¥¼ ìˆ˜ì •í•˜ì„¸ìš”.\n",
    "        \n",
    "        [ê¸°ì¡´ ì½”ë“œ]\n",
    "        {code}\n",
    "        \n",
    "        [ë¦¬ë·°ì–´ í”¼ë“œë°±]\n",
    "        {critique}\n",
    "        \n",
    "        í”¼ë“œë°±ì„ ë°˜ì˜í•˜ì—¬ ê°œì„ ëœ 'ì „ì²´ ì½”ë“œ'ë§Œ ë‹¤ì‹œ ì¶œë ¥í•˜ì„¸ìš”. (ì„¤ëª… ì œì™¸)\n",
    "        \"\"\"\n",
    "    ) | llm | StrOutputParser()\n",
    "    \n",
    "    new_code = chain.invoke({\n",
    "        \"code\": state[\"code_result\"],\n",
    "        \"critique\": state[\"critique\"]\n",
    "    })\n",
    "    if \"run_id\" in state:\n",
    "        save_step_to_file(state[\"run_id\"], \"Code_Done\", {\"summary\": new_code})\n",
    "    \n",
    "    return {\n",
    "        \"code_result\": new_code,\n",
    "        \"retry_count\": state[\"retry_count\"] + 1,\n",
    "        \"logs\": [AIMessage(content=f\"ì½”ë“œ ìˆ˜ì • ì™„ë£Œ (ì‹œë„ {state['retry_count']+1}íšŒ)\", name=\"coder\")]\n",
    "    }\n",
    "    \n",
    "code_workflow = StateGraph(CodeState)\n",
    "\n",
    "code_workflow.add_node(\"execute\", code_execute_node)\n",
    "code_workflow.add_node(\"reflect\", code_reflect_node)\n",
    "code_workflow.add_node(\"revise\", code_revise_node)\n",
    "\n",
    "# íë¦„ ì •ì˜\n",
    "code_workflow.add_edge(START, \"execute\")\n",
    "code_workflow.add_edge(\"execute\", \"reflect\")\n",
    "\n",
    "# ë¶„ê¸° ë¡œì§ (Conditional Edge)\n",
    "def route_code(state: CodeState):\n",
    "    # í†µê³¼í–ˆê±°ë‚˜, ì¬ì‹œë„ íšŸìˆ˜ê°€ 3íšŒë¥¼ ë„˜ìœ¼ë©´ ì¢…ë£Œ\n",
    "    if state[\"quality\"] == \"PASS\" or state[\"retry_count\"] >= 3:\n",
    "        return END\n",
    "    return \"revise\" # ì‹¤íŒ¨í•˜ë©´ ìˆ˜ì •í•˜ëŸ¬ ì´ë™\n",
    "\n",
    "code_workflow.add_conditional_edges(\n",
    "    \"reflect\", \n",
    "    route_code, \n",
    "    {\"revise\": \"revise\", END: END}\n",
    ")\n",
    "\n",
    "code_workflow.add_edge(\"revise\", \"reflect\")\n",
    "\n",
    "code_app = code_workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a2a8eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DesignerState(TypedDict):\n",
    "    topic: str\n",
    "    logs: Annotated[List[BaseMessage], add_messages]\n",
    "    design_result: str # Mermaid ì½”ë“œ ì €ì¥\n",
    "    critique: str      # í‰ê°€ í”¼ë“œë°± ì €ì¥\n",
    "    quality: str       # PASS / FAIL\n",
    "    retry_count: int\n",
    "    \n",
    "def designer_execute_node(state: DesignerState):\n",
    "    print(f\"[Designer Agent] '{state['topic']}' ì‹œê°í™” êµ¬ì¡° ì„¤ê³„ ì¤‘...\")\n",
    "    \n",
    "    # Mermaid ì½”ë“œ ìƒì„± í”„ë¡¬í”„íŠ¸\n",
    "    chain = ChatPromptTemplate.from_template(\n",
    "        \"\"\"ë‹¹ì‹ ì€ ì‹œìŠ¤í…œ ì•„í‚¤í…íŠ¸ì…ë‹ˆë‹¤. \n",
    "        ì£¼ì œ '{topic}'ì˜ êµ¬ì¡°ë‚˜ íë¦„ì„ ê°€ì¥ ì˜ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” 'Mermaid ë‹¤ì´ì–´ê·¸ë¨' ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”.\n",
    "        \n",
    "        [ìš”êµ¬ì‚¬í•­]\n",
    "        1. íë¦„ë„(graph TD) ë˜ëŠ” ì‹œí€€ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨(sequenceDiagram) ì¤‘ ì ì ˆí•œ ê²ƒì„ ì„ íƒí•˜ì„¸ìš”.\n",
    "        2. ì„¤ëª… í…ìŠ¤íŠ¸ ì—†ì´ ì˜¤ì§ Mermaid ì½”ë“œë§Œ ì¶œë ¥í•˜ì„¸ìš”.\n",
    "        3. ë§ˆí¬ë‹¤ìš´ íƒœê·¸(```mermaid)ëŠ” ì œì™¸í•˜ê³  ìˆœìˆ˜ ì½”ë“œë§Œ ì£¼ì„¸ìš”.\n",
    "        \"\"\"\n",
    "    ) | llm | StrOutputParser()\n",
    "    \n",
    "    design = chain.invoke({\"topic\": state[\"topic\"]})\n",
    "    \n",
    "    if \"run_id\" in state:\n",
    "        save_step_to_file(state[\"run_id\"], \"Design_Done\", {\"summary\": final_summary})\n",
    "        \n",
    "    return {\n",
    "        \"design_result\": design,\n",
    "        \"retry_count\": 0,\n",
    "        \"logs\": [AIMessage(content=\"ë‹¤ì´ì–´ê·¸ë¨ ì´ˆì•ˆ ìƒì„± ì™„ë£Œ\", name=\"designer\")]\n",
    "    }\n",
    "    \n",
    "def designer_reflect_node(state: DesignerState):\n",
    "    print(\"[Designer Agent] ë‹¤ì´ì–´ê·¸ë¨ ë¬¸ë²• ë° ì ì ˆì„± ê²€ì‚¬ ì¤‘...\")\n",
    "    \n",
    "    # í‰ê°€ í”„ë¡¬í”„íŠ¸\n",
    "    chain = ChatPromptTemplate.from_template(\n",
    "        \"\"\"ë‹¹ì‹ ì€ Mermaid ë¬¸ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤. \n",
    "        ì•„ë˜ ì½”ë“œê°€ ë¬¸ë²•ì ìœ¼ë¡œ ì˜¬ë°”ë¥´ê³  ì£¼ì œë¥¼ ì˜ í‘œí˜„í•˜ëŠ”ì§€ ê²€ì‚¬í•˜ì„¸ìš”.\n",
    "        \n",
    "        [ê²€í† í•  ì½”ë“œ]\n",
    "        {code}\n",
    "        \n",
    "        [ì¶œë ¥ í˜•ì‹]\n",
    "        ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”:\n",
    "        ìƒíƒœ: [PASS ë˜ëŠ” FAIL]\n",
    "        í”¼ë“œë°±: [ì˜¤ë¥˜ ë‚´ìš© ë˜ëŠ” ê°œì„ ì ]\n",
    "        \"\"\"\n",
    "    ) | llm | StrOutputParser()\n",
    "    \n",
    "    review_result = chain.invoke({\"code\": state[\"design_result\"]})\n",
    "    \n",
    "    # ê²°ê³¼ íŒŒì‹±\n",
    "    try:\n",
    "        status_line = review_result.split(\"\\n\")[0]\n",
    "        quality = \"PASS\" if \"PASS\" in status_line else \"FAIL\"\n",
    "        critique = review_result\n",
    "    except:\n",
    "        quality = \"FAIL\"\n",
    "        critique = \"í˜•ì‹ ì˜¤ë¥˜ ë°œìƒ\"\n",
    "        \n",
    "    print(f\"      ã„´ ê²€ì‚¬ ê²°ê³¼: {quality}\")\n",
    "    if quality == \"FAIL\":\n",
    "        print(f\"      ã„´í”¼ë“œë°±: {critique.split('í”¼ë“œë°±:')[1][:50]}...\")\n",
    "\n",
    "    return {\n",
    "        \"quality\": quality,\n",
    "        \"critique\": critique,\n",
    "        \"logs\": [AIMessage(content=f\"ê²€ì‚¬ ì™„ë£Œ: {quality}\", name=\"reviewer\")]\n",
    "    }\n",
    "    \n",
    "def designer_revise_node(state: DesignerState):\n",
    "    print(\"[Designer Agent] í”¼ë“œë°± ë°˜ì˜í•˜ì—¬ ìˆ˜ì • ì¤‘...\")\n",
    "    \n",
    "    chain = ChatPromptTemplate.from_template(\n",
    "        \"\"\"ë‹¹ì‹ ì€ ë””ìì´ë„ˆì…ë‹ˆë‹¤. í”¼ë“œë°±ì„ ë°˜ì˜í•˜ì—¬ Mermaid ì½”ë“œë¥¼ ìˆ˜ì •í•˜ì„¸ìš”.\n",
    "        \n",
    "        [ê¸°ì¡´ ì½”ë“œ]\n",
    "        {code}\n",
    "        \n",
    "        [í”¼ë“œë°±]\n",
    "        {critique}\n",
    "        \n",
    "        ìˆ˜ì •ëœ ì „ì²´ Mermaid ì½”ë“œë§Œ ì¶œë ¥í•˜ì„¸ìš”. (ì„¤ëª… ì œì™¸)\n",
    "        \"\"\"\n",
    "    ) | llm | StrOutputParser()\n",
    "    \n",
    "    new_design = chain.invoke({\n",
    "        \"code\": state[\"design_result\"],\n",
    "        \"critique\": state[\"critique\"]\n",
    "    })\n",
    "    \n",
    "    return {\n",
    "        \"design_result\": new_design,\n",
    "        \"retry_count\": state[\"retry_count\"] + 1,\n",
    "        \"logs\": [AIMessage(content=f\"ìˆ˜ì • ì™„ë£Œ (ì‹œë„ {state['retry_count']+1}íšŒ)\", name=\"designer\")]\n",
    "    }\n",
    "    \n",
    "designer_workflow = StateGraph(DesignerState)\n",
    "\n",
    "designer_workflow.add_node(\"execute\", designer_execute_node)\n",
    "designer_workflow.add_node(\"reflect\", designer_reflect_node)\n",
    "designer_workflow.add_node(\"revise\", designer_revise_node)\n",
    "\n",
    "# íë¦„ ì—°ê²°\n",
    "designer_workflow.add_edge(START, \"execute\")\n",
    "designer_workflow.add_edge(\"execute\", \"reflect\")\n",
    "\n",
    "# ë¶„ê¸° ë¡œì§\n",
    "def route_design(state: DesignerState):\n",
    "    if state[\"quality\"] == \"PASS\" or state[\"retry_count\"] >= 3:\n",
    "        return END\n",
    "    return \"revise\"\n",
    "\n",
    "designer_workflow.add_conditional_edges(\n",
    "    \"reflect\", \n",
    "    route_design, \n",
    "    {\"revise\": \"revise\", END: END}\n",
    ")\n",
    "\n",
    "# ë£¨í”„ ì—°ê²°\n",
    "designer_workflow.add_edge(\"revise\", \"reflect\")\n",
    "\n",
    "designer_app = designer_workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9b43e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_agent_results(existing: Dict[str, Any], new_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    # ê¸°ì¡´ ë”•ì…”ë„ˆë¦¬ì— ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ì—…ë°ì´íŠ¸(ë³‘í•©)\n",
    "    if existing is None:\n",
    "        return new_data\n",
    "    # ì–•ì€ ë³µì‚¬ í›„ ì—…ë°ì´íŠ¸í•˜ì—¬ ê¸°ì¡´ ë°ì´í„° ë³´ì¡´\n",
    "    merged = existing.copy()\n",
    "    merged.update(new_data)\n",
    "    return merged\n",
    "\n",
    "\n",
    "class MainState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    agent_results: Annotated[Dict[str, Any], update_agent_results]\n",
    "    next: List[str]\n",
    "\n",
    "# 2-1. Supervisor (LLM ê²°ì •)\n",
    "class SupervisorDecision(BaseModel):\n",
    "    next: List[Literal['research_subgraph', 'code_subgraph', 'designer_subgraph', 'writer_subgraph', 'FINISH']] = Field(\n",
    "        description=\"ë‹¤ìŒì— ì‹¤í–‰í•  ì—ì´ì „íŠ¸ ëª©ë¡. ë³‘ë ¬ ì‹¤í–‰ì´ í•„ìš”í•˜ë©´ ì—¬ëŸ¬ ê°œë¥¼ ì„ íƒí•˜ì„¸ìš”.\"\n",
    "    )\n",
    "    reasoning: str = Field(description=\"ì´ ê²°ì •ì„ ë‚´ë¦° ì´ìœ  (ì„±ì°°)\")\n",
    "\n",
    "\n",
    "def supervisor_node(state: MainState):\n",
    "    results = state.get(\"agent_results\", {})\n",
    "    messages = state.get(\"messages\", [])\n",
    "    last_user_msg = messages[-1].content if messages else \"\"\n",
    "    \n",
    "    status = {\n",
    "        \"research\": \"ìˆìŒ\" if \"research\" in results else \"ì—†ìŒ\",\n",
    "        \"code\": \"ìˆìŒ\" if \"code\" in results else \"ì—†ìŒ\",\n",
    "        \"design\": \"ìˆìŒ\" if \"design\" in results else \"ì—†ìŒ\",\n",
    "        \"final_doc\": \"ìˆìŒ\" if \"final_doc\" in results else \"ì—†ìŒ\"\n",
    "    }\n",
    "    \n",
    "    # ê·œì¹™: ì—°êµ¬ìë£Œ ì—†ìœ¼ë©´ research -> ìˆìœ¼ë©´ writer_team -> ëë‚˜ë©´ FINISH\n",
    "    system_prompt = f\"\"\"ë‹¹ì‹ ì€ ìœ ëŠ¥í•œ AI í”„ë¡œì íŠ¸ ë§¤ë‹ˆì €ì…ë‹ˆë‹¤. \n",
    "    ì‚¬ìš©ìì˜ ìš”ì²­ê³¼ í˜„ì¬ ì‘ì—… ìƒíƒœë¥¼ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ì‘ì—…ì(ë“¤)ì„ ì§€ì •í•˜ì„¸ìš”.\n",
    "    \n",
    "    [ì‚¬ìš©ì ìš”ì²­]: \"{last_user_msg}\"\n",
    "    \n",
    "    [í˜„ì¬ ë°ì´í„° ìƒíƒœ]\n",
    "    {status}\n",
    "    \n",
    "    [íŒë‹¨ ê°€ì´ë“œ]\n",
    "    1. ì½”ë“œ/ë””ìì¸ í•„ìš”ì„± íŒë‹¨:\n",
    "       - ìš”ì²­ì´ 'êµ¬í˜„', 'ê°œë°œ', 'ì„¤ê³„', 'ì•Œê³ ë¦¬ì¦˜', 'êµ¬ì¡°ë„' ë“±ì„ í¬í•¨í•˜ë‚˜ìš”? -> Code/Designer í˜¸ì¶œ\n",
    "       - ë‹¨ìˆœ 'ë™í–¥ íŒŒì•…', 'ë¶„ì„ ë³´ê³ ì„œ', 'ì—ì„¸ì´'ì¸ê°€ìš”? -> Researchë§Œ í˜¸ì¶œ (Code/Design ìƒëµ)\n",
    "    \n",
    "    2. ì‘ì—… ìˆœì„œ:\n",
    "       1. **0ìˆœìœ„: ë¬´ì¡°ê±´ ì¢…ë£Œ**:\n",
    "       - 'Final Document' ìƒíƒœê°€ 'ìˆìŒ'ì´ë¼ë©´, ë‹¤ë¥¸ ì¡°ê±´ ë³¼ ê²ƒ ì—†ì´ ë¬´ì¡°ê±´ 'FINISH'ë¥¼ ì„ íƒí•˜ì„¸ìš”. (ì ˆëŒ€ Writerë¥¼ ë‹¤ì‹œ ë¶€ë¥´ì§€ ë§ˆì„¸ìš”)\n",
    "    \n",
    "    2. **ì¤‘ë³µ ì‹¤í–‰ ê¸ˆì§€**: \n",
    "       - ì´ë¯¸ 'Code ê²°ê³¼'ê°€ 'ìˆìŒ'ì´ë¼ë©´, ì ˆëŒ€ë¡œ 'code_subgraph'ë¥¼ ë‹¤ì‹œ í˜¸ì¶œí•˜ì§€ ë§ˆì„¸ìš”.\n",
    "       - ì´ë¯¸ 'Research ê²°ê³¼'ê°€ 'ìˆìŒ'ì´ë¼ë©´, ì ˆëŒ€ë¡œ 'research_subgraph'ë¥¼ ë‹¤ì‹œ í˜¸ì¶œí•˜ì§€ ë§ˆì„¸ìš”.\n",
    "    \n",
    "    3. **ì‘ì—… íë¦„**:\n",
    "       - (1ë‹¨ê³„) ìë£Œ ìƒì„±: ìš”ì²­ì— ë”°ë¼ Research, Code, Design íŒ€ì„ í˜¸ì¶œí•©ë‹ˆë‹¤.\n",
    "       - (2ë‹¨ê³„) ë¬¸ì„œ ì‘ì„±: ìœ„ ìë£Œë“¤ì´ ì¤€ë¹„ë˜ì—ˆê³ , ì•„ì§ 'final_doc'ê°€ 'ì—†ìŒ'ì´ë¼ë©´ -> 'writer_subgraph'ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”.\n",
    "       \n",
    "    4. **íŠ¹ìˆ˜ ìƒí™©**:\n",
    "       - ë§Œì•½ ì‚¬ìš©ìê°€ ì½”ë“œë§Œ ìš”ì²­í–ˆê³  'Code ê²°ê³¼'ëŠ” ìˆëŠ”ë° 'Final Document'ê°€ ì—†ë‹¤ë©´ -> 'writer_subgraph'ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”.\n",
    "    3. ë³‘ë ¬ ì‹¤í–‰:\n",
    "       - ì—°êµ¬, ì½”ë“œ, ë””ìì¸ì´ ëª¨ë‘ í•„ìš”í•˜ë‹¤ê³  íŒë‹¨ë˜ë©´ ë™ì‹œì— í˜¸ì¶œí•˜ì„¸ìš”.\n",
    "       \n",
    "    \"\"\"\n",
    "    \n",
    "    model = llm.with_structured_output(SupervisorDecision)\n",
    "    decision = model.invoke([SystemMessage(content=system_prompt)])\n",
    "    \n",
    "    print(f\"\\n[Main Supervisor] ì§€ì‹œ: {decision.next}\")\n",
    "    return {\"next\": decision.next}\n",
    "\n",
    "def call_research_subgraph(state: MainState):\n",
    "    print(\"[Main] 'Research ì„œë¸Œê·¸ë˜í”„' í˜¸ì¶œ\")\n",
    "    topic = state[\"messages\"][0].content # ì‚¬ìš©ì ì§ˆë¬¸\n",
    "    \n",
    "    # ì„œë¸Œê·¸ë˜í”„ ì‹¤í–‰\n",
    "    output = research_app.invoke({\"topic\": topic})\n",
    "    \n",
    "    return {\"agent_results\": {\"research\": output[\"raw_data\"]}}\n",
    "\n",
    "def call_writer_subgraph(state: MainState):\n",
    "    print(\"\\n[Main] 'Writer ì„œë¸Œê·¸ë˜í”„' í˜¸ì¶œ\")\n",
    "    \n",
    "    topic = state[\"messages\"][0].content\n",
    "    results = state[\"agent_results\"]\n",
    "    \n",
    "    output = writer_app.invoke({\n",
    "        \"topic\": topic, \n",
    "        \"research_data\": results.get(\"research\", \"\"),\n",
    "        \"code_data\": results.get(\"code\", \"\"),\n",
    "        \"design_data\": results.get(\"design\", \"\"),\n",
    "        \"revision_count\": 0\n",
    "    })\n",
    "    \n",
    "    # ìµœì¢… ë¬¸ì„œë§Œ ë¦¬í„´\n",
    "    return {\"agent_results\": {\"final_doc\": output[\"draft\"]}}\n",
    "\n",
    "def call_code_subgraph(state: MainState):\n",
    "    print(\"[Main] 'Code íŒ€' (ì„œë¸Œê·¸ë˜í”„) í˜¸ì¶œ\")\n",
    "    \n",
    "    topic = state[\"messages\"][0].content\n",
    "    \n",
    "    output = code_app.invoke({\n",
    "        \"topic\": topic,\n",
    "        \"retry_count\": 0 \n",
    "    })\n",
    "    \n",
    "    return {\"agent_results\": {\"code\": output[\"code_result\"]}}\n",
    "\n",
    "def call_designer_subgraph(state: MainState):\n",
    "    print(\"[Main] 'Designer íŒ€' (ì„œë¸Œê·¸ë˜í”„) í˜¸ì¶œ\")\n",
    "    \n",
    "    topic = state[\"messages\"][0].content\n",
    "    \n",
    "    # Designer Subgraph ì‹¤í–‰\n",
    "    output = designer_app.invoke({\n",
    "        \"topic\": topic,\n",
    "        \"retry_count\": 0\n",
    "    })\n",
    "    print(f\"[Main] Designer íŒ€ ì‘ì—… ì™„ë£Œ\")\n",
    "    \n",
    "    return {\"agent_results\": {\"design\": output[\"design_result\"]}}\n",
    "\n",
    "main_workflow = StateGraph(MainState)\n",
    "main_workflow.add_node(\"supervisor\", supervisor_node)\n",
    "main_workflow.add_node(\"research_subgraph\", call_research_subgraph)\n",
    "main_workflow.add_node(\"writer_subgraph\", call_writer_subgraph)\n",
    "main_workflow.add_node(\"code_subgraph\", call_code_subgraph)\n",
    "main_workflow.add_node(\"designer_subgraph\", call_designer_subgraph)\n",
    "\n",
    "main_workflow.add_edge(START, \"supervisor\")\n",
    "main_workflow.add_edge(\"research_subgraph\", \"supervisor\")\n",
    "main_workflow.add_edge(\"writer_subgraph\", \"supervisor\")\n",
    "main_workflow.add_edge(\"code_subgraph\", \"supervisor\")\n",
    "main_workflow.add_edge(\"designer_subgraph\", \"supervisor\")\n",
    "\n",
    "def route_supervisor(state: MainState):\n",
    "    next_agents = state[\"next\"]\n",
    "    \n",
    "    # ì¢…ë£Œ ì¡°ê±´\n",
    "    if \"FINISH\" in next_agents:\n",
    "        return END\n",
    "    \n",
    "    # ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ -> LangGraphê°€ ìë™ìœ¼ë¡œ ë³‘ë ¬ ì‹¤í–‰í•¨\n",
    "    return next_agents\n",
    "\n",
    "# Supervisorì˜ 'next' ê°’ì— ë”°ë¼ ë¶„ê¸°\n",
    "main_workflow.add_conditional_edges(\n",
    "    \"supervisor\",\n",
    "    route_supervisor,\n",
    "    # ë§¤í•‘ì€ ëª…ì‹œì ìœ¼ë¡œ ëª¨ë“  ê°€ëŠ¥ì„±ì„ ì—´ì–´ë‘¡ë‹ˆë‹¤\n",
    "    {\n",
    "        \"research_subgraph\": \"research_subgraph\",\n",
    "        \"code_subgraph\": \"code_subgraph\",\n",
    "        \"designer_subgraph\": \"designer_subgraph\",\n",
    "        \"writer_subgraph\": \"writer_subgraph\",\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = main_workflow.compile(checkpointer=memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b967625a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š [RAG ì—”ì§„] DB ê²½ë¡œ í™•ì¸: ./chroma_recur_db\n",
      "   âœ… ê¸°ì¡´ 'chroma_recur_db'ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ë¡œë“œ ì¤‘...\n",
      "âœ… [RAG ì—”ì§„] ì¤€ë¹„ ì™„ë£Œ!\n",
      "\n",
      "==================================================\n",
      "ğŸ® [RAG íŒŒì´í”„ë¼ì¸] ëª…ë ¹ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”.\n",
      "1. ìš”ì²­: req <run_id> <ì§ˆë¬¸>  (ì˜ˆ: req run_1 RAGì˜ ì¥ì ì€?)\n",
      "2. ì¤‘ë‹¨: stop\n",
      "3. ì¬ê°œ: resume <run_id> <ì§ˆë¬¸>\n",
      "4. ì¢…ë£Œ: exit\n",
      "==================================================\n",
      "\n",
      "ğŸ‘· [ì›Œì»¤] ê°€ë™ ì¤‘... (RAG ì—”ì§„ ì¤€ë¹„ë¨)\n",
      "ğŸ“¥ [ì‹œìŠ¤í…œ] ìš”ì²­ ì ‘ìˆ˜: test_01\n",
      "\n",
      "ğŸš€ [ì‹œì‘] test_01: RAGì˜ ì¥ì ì´ ë­ì•¼?\n",
      "   â–¶ [ì‹¤í–‰] Step 1: ì§ˆë¬¸ ë¶„ì„ ë° ê³„íš\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import builtins\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "\n",
    "# --- LangChain & RAG ê´€ë ¨ ì„í¬íŠ¸ ---\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# ğŸš¨ ì¶œë ¥ ë²„í¼ë§ í•´ì œ (ì‹¤ì‹œê°„ ë¡œê·¸ìš©)\n",
    "builtins.print = partial(builtins.print, flush=True)\n",
    "\n",
    "# ==========================================\n",
    "# 1. RAG ì—”ì§„ (ê¸°ì¡´ DB ì—°ë™)\n",
    "# ==========================================\n",
    "class RAGEngine:\n",
    "    def __init__(self, pdf_dir=\"./data\", persist_dir=\"./chroma_recur_db\"): # ì‚¬ìš©ì í´ë”ëª… ë°˜ì˜\n",
    "        self.pdf_dir = pdf_dir\n",
    "        self.persist_dir = persist_dir\n",
    "        self.collection_name = 'recur_chunks_collection' # ì‚¬ìš©ì ì»¬ë ‰ì…˜ëª… ë°˜ì˜\n",
    "        \n",
    "        # 3ì£¼ì°¨ ì½”ë“œì™€ ë™ì¼í•œ ëª¨ë¸ ì„¤ì •\n",
    "        self.llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "        self.embeddings = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "        \n",
    "        self.vectorstore = None\n",
    "        self.retriever = None\n",
    "        \n",
    "        # ì´ˆê¸°í™” ì‹¤í–‰\n",
    "        self._initialize_db()\n",
    "\n",
    "    def _initialize_db(self):\n",
    "        \"\"\"ê¸°ì¡´ DBê°€ ìˆìœ¼ë©´ ë¡œë“œí•˜ê³ , ì—†ìœ¼ë©´ ìƒˆë¡œ ë§Œë“­ë‹ˆë‹¤.\"\"\"\n",
    "        print(f\"ğŸ“š [RAG ì—”ì§„] DB ê²½ë¡œ í™•ì¸: {self.persist_dir}\")\n",
    "        \n",
    "        # 1. ê¸°ì¡´ DB í´ë”ê°€ ìˆëŠ”ì§€ í™•ì¸\n",
    "        if os.path.exists(self.persist_dir) and os.listdir(self.persist_dir):\n",
    "            print(\"   âœ… ê¸°ì¡´ 'chroma_recur_db'ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ë¡œë“œ ì¤‘...\")\n",
    "            self.vectorstore = Chroma(\n",
    "                persist_directory=self.persist_dir,\n",
    "                embedding_function=self.embeddings,\n",
    "                collection_name=self.collection_name\n",
    "            )\n",
    "        else:\n",
    "            # 2. ì—†ìœ¼ë©´ ìƒˆë¡œ êµ¬ì¶• (ì˜ˆì™¸ ì²˜ë¦¬)\n",
    "            print(\"   âš ï¸ ê¸°ì¡´ DBê°€ ì—†ìŠµë‹ˆë‹¤. ë¬¸ì„œë¥¼ ìƒˆë¡œ ë¡œë“œí•©ë‹ˆë‹¤...\")\n",
    "            docs = []\n",
    "            if os.path.exists(self.pdf_dir):\n",
    "                for f in os.listdir(self.pdf_dir):\n",
    "                    if f.endswith('.pdf'):\n",
    "                        print(f\"      - ë¡œë“œ ì¤‘: {f}\")\n",
    "                        loader = PyPDFLoader(os.path.join(self.pdf_dir, f))\n",
    "                        docs.extend(loader.load())\n",
    "            \n",
    "            if not docs:\n",
    "                print(\"      (PDF íŒŒì¼ì´ ì—†ì–´ í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤)\")\n",
    "                from langchain_core.documents import Document\n",
    "                docs = [Document(page_content=\"RAGëŠ” ê²€ìƒ‰ ì¦ê°• ìƒì„± ê¸°ìˆ ì…ë‹ˆë‹¤.\", metadata={\"source\": \"dummy\"})]\n",
    "\n",
    "            # 3ì£¼ì°¨ ì½”ë“œì˜ Splitter ì„¤ì • ë°˜ì˜\n",
    "            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "            splits = text_splitter.split_documents(docs)\n",
    "            \n",
    "            self.vectorstore = Chroma.from_documents(\n",
    "                documents=splits,\n",
    "                embedding=self.embeddings,\n",
    "                collection_name=self.collection_name,\n",
    "                persist_directory=self.persist_dir\n",
    "            )\n",
    "            \n",
    "        # Retriever ì„¤ì •\n",
    "        self.retriever = self.vectorstore.as_retriever(search_kwargs={'k': 3})\n",
    "        print(\"âœ… [RAG ì—”ì§„] ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "\n",
    "    async def retrieve(self, query):\n",
    "        \"\"\"ë¹„ë™ê¸° ë¬¸ì„œ ê²€ìƒ‰\"\"\"\n",
    "        # invoke ëŒ€ì‹  ainvoke ì‚¬ìš© (ë¹„ë™ê¸°)\n",
    "        docs = await self.retriever.ainvoke(query)\n",
    "        return docs\n",
    "\n",
    "    async def generate_answer(self, query, context_docs):\n",
    "        \"\"\"ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ ìƒì„±\"\"\"\n",
    "        context_text = \"\\n\\n\".join([doc.page_content for doc in context_docs])\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"ë‹¹ì‹ ì€ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì•„ë˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.\n",
    "            ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ 'ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µí•˜ì„¸ìš”.\n",
    "            \n",
    "            ì»¨í…ìŠ¤íŠ¸:\n",
    "            {context}\n",
    "            \n",
    "            ì§ˆë¬¸: {question}\n",
    "            \"\"\"\n",
    "        )\n",
    "        chain = prompt | self.llm | StrOutputParser()\n",
    "        return await chain.ainvoke({\"context\": context_text, \"question\": query})\n",
    "\n",
    "# ==========================================\n",
    "# 2. íŒŒì´í”„ë¼ì¸ í”„ë¡œì„¸ì„œ (ë‚˜ë¨¸ì§€ëŠ” ë™ì¼)\n",
    "# ==========================================\n",
    "\n",
    "class Step(Enum):\n",
    "    PLAN = 1      # ê³„íš\n",
    "    READ = 2      # ê²€ìƒ‰ (Retrieval)\n",
    "    DRAFT = 3     # ìƒì„± (Generation)\n",
    "    FINALIZE = 4  # ì •ë¦¬\n",
    "\n",
    "STEP_INFO = {\n",
    "    Step.PLAN: \"ì§ˆë¬¸ ë¶„ì„ ë° ê³„íš\",\n",
    "    Step.READ: \"ë¬¸ì„œ ê²€ìƒ‰ (Retrieval)\",\n",
    "    Step.DRAFT: \"ë‹µë³€ ìƒì„± (Generation)\",\n",
    "    Step.FINALIZE: \"ìµœì¢… ê²°ê³¼ í¬ë§·íŒ…\"\n",
    "}\n",
    "\n",
    "def save_step_to_file(run_id, step_num, step_name, result):\n",
    "    directory = f\"runs/{run_id}\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    filename = f\"{directory}/step_{step_num:02d}.json\"\n",
    "    \n",
    "    # ì§ë ¬í™” ì²˜ë¦¬\n",
    "    serializable_result = result\n",
    "    if hasattr(result, 'page_content'): \n",
    "        serializable_result = result.page_content\n",
    "    elif isinstance(result, list): \n",
    "        serializable_result = [d.page_content if hasattr(d, 'page_content') else str(d) for d in result]\n",
    "\n",
    "    data = {\n",
    "        \"step\": step_num,\n",
    "        \"step_name\": step_name,\n",
    "        \"result\": serializable_result,\n",
    "        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    }\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"   ğŸ’¾ [íŒŒì¼ ì €ì¥] {filename}\")\n",
    "\n",
    "def load_checkpoint(run_id):\n",
    "    path = f\"runs/{run_id}/checkpoint.json\"\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    return None\n",
    "\n",
    "def save_checkpoint(run_id, step_num, user_goal, context_data):\n",
    "    directory = f\"runs/{run_id}\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    # ğŸŒŸ [í•µì‹¬] Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (JSON ì €ì¥ìš©)\n",
    "    serializable_context = context_data\n",
    "    if isinstance(context_data, list) and context_data:\n",
    "        # ë‚´ìš©ë¬¼ì´ Document ê°ì²´ì¸ì§€ í™•ì¸\n",
    "        if hasattr(context_data[0], 'page_content'):\n",
    "            serializable_context = [\n",
    "                {\"page_content\": doc.page_content, \"metadata\": doc.metadata} \n",
    "                for doc in context_data\n",
    "            ]\n",
    "\n",
    "    data = {\n",
    "        \"run_id\": run_id,\n",
    "        \"completed_step\": step_num,\n",
    "        \"user_goal\": user_goal,\n",
    "        \"context_data\": serializable_context # ë³€í™˜ëœ ë°ì´í„° ì €ì¥\n",
    "    }\n",
    "    \n",
    "    with open(f\"{directory}/checkpoint.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "class PipelineProcessor:\n",
    "    def __init__(self):\n",
    "        self.rag_engine = RAGEngine()\n",
    "        self.stop_flag = False\n",
    "\n",
    "    def stop(self):\n",
    "        self.stop_flag = True\n",
    "        print(\"\\nğŸ›‘ [ì‹œìŠ¤í…œ] STOP ëª…ë ¹ ìˆ˜ì‹ ! (ë‹¨ê³„ ì™„ë£Œ í›„ ë©ˆì¶¤)\")\n",
    "\n",
    "    async def run_pipeline(self, run_id, user_goal, resume=False):\n",
    "        self.stop_flag = False\n",
    "        start_step = 1\n",
    "        current_data = None \n",
    "\n",
    "        # --- Resume ë¡œì§ (ë°ì´í„° ë³µì›) ---\n",
    "        if resume:\n",
    "            ckpt = load_checkpoint(run_id)\n",
    "            if ckpt:\n",
    "                start_step = ckpt['completed_step'] + 1\n",
    "                loaded_data = ckpt.get('context_data')\n",
    "                \n",
    "                # ğŸŒŸ [í•µì‹¬] ë”•ì…”ë„ˆë¦¬ë¥¼ ë‹¤ì‹œ Document ê°ì²´ë¡œ ë³µì›\n",
    "                if isinstance(loaded_data, list) and loaded_data:\n",
    "                    # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¼ë©´ Document ê°ì²´ë¡œ ë³€í™˜\n",
    "                    if isinstance(loaded_data[0], dict) and 'page_content' in loaded_data[0]:\n",
    "                        current_data = [\n",
    "                            Document(page_content=d['page_content'], metadata=d.get('metadata', {})) \n",
    "                            for d in loaded_data\n",
    "                        ]\n",
    "                        print(f\"   ğŸ”„ [ì‹œìŠ¤í…œ] ì €ì¥ëœ ë¬¸ì„œ {len(current_data)}ê°œë¥¼ ë©”ëª¨ë¦¬ë¡œ ë³µì›í–ˆìŠµë‹ˆë‹¤.\")\n",
    "                    else:\n",
    "                        current_data = loaded_data\n",
    "                else:\n",
    "                    current_data = loaded_data\n",
    "\n",
    "                print(f\"ğŸ”„ [Resume] Step {start_step}ë¶€í„° ì‘ì—…ì„ ì´ì–´ê°‘ë‹ˆë‹¤.\")\n",
    "            else:\n",
    "                print(\"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ. ì²˜ìŒë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "        # --- ë©”ì¸ ë£¨í”„ (Step 1~4) ---\n",
    "        for step_num in range(start_step, 5):\n",
    "            if self.stop_flag:\n",
    "                print(f\"ğŸ›‘ Step {step_num} ì§„ì… ì „ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "                return \n",
    "\n",
    "            step_enum = Step(step_num)\n",
    "            step_name = STEP_INFO[step_enum]\n",
    "            \n",
    "            print(f\"   â–¶ [ì‹¤í–‰] Step {step_num}: {step_name}\")\n",
    "            \n",
    "            output_data = None\n",
    "            \n",
    "            if step_enum == Step.PLAN:\n",
    "                output_data = f\"ì§ˆë¬¸ '{user_goal}'ì— ëŒ€í•´ ê¸°ì¡´ DBë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.\"\n",
    "                await asyncio.sleep(0.5)\n",
    "                \n",
    "            elif step_enum == Step.READ:\n",
    "                print(f\"      ğŸ” '{user_goal}' ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...\")\n",
    "                docs = await self.rag_engine.retrieve(user_goal)\n",
    "                output_data = docs\n",
    "                print(f\"      ğŸ“„ ê´€ë ¨ ë¬¸ì„œ {len(docs)}ê°œ ë°œê²¬\")\n",
    "                \n",
    "            elif step_enum == Step.DRAFT:\n",
    "                # ë°ì´í„°ê°€ ìˆìœ¼ë©´(Resume ë“±) ì‚¬ìš©, ì—†ìœ¼ë©´ ê²€ìƒ‰\n",
    "                if current_data: \n",
    "                    context_docs = current_data \n",
    "                    print(f\"      ğŸ“š [ê¸°ì–µ í™œìš©] ì €ì¥ëœ ë¬¸ì„œ {len(context_docs)}ê°œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "                else:\n",
    "                    context_docs = await self.rag_engine.retrieve(user_goal)\n",
    "                    \n",
    "                print(f\"      âœï¸ LLMì´ ë‹µë³€ ì‘ì„± ì¤‘...\")\n",
    "                answer = await self.rag_engine.generate_answer(user_goal, context_docs)\n",
    "                output_data = answer\n",
    "                \n",
    "            elif step_enum == Step.FINALIZE:\n",
    "                final_text = f\"=== ìµœì¢… ë‹µë³€ ===\\n\\n[ì§ˆë¬¸]: {user_goal}\\n\\n[ë‚´ìš©]:\\n{current_data}\\n\\n[Generated by RAG Agent]\"\n",
    "                output_data = final_text\n",
    "\n",
    "            # ê²°ê³¼ ì €ì¥\n",
    "            save_step_to_file(run_id, step_num, step_name, output_data)\n",
    "            \n",
    "            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ê²€ìƒ‰ëœ ë¬¸ì„œ ë“± ì¤‘ìš”í•œ ë°ì´í„°ë¥¼ ì €ì¥)\n",
    "            save_data_for_next = output_data\n",
    "            save_checkpoint(run_id, step_num, user_goal, save_data_for_next)\n",
    "            \n",
    "            current_data = output_data\n",
    "\n",
    "        if not self.stop_flag:\n",
    "            print(f\"âœ… [ì™„ë£Œ] Run ID: {run_id}\")\n",
    "\n",
    "class AgentQueueSystem:\n",
    "    def __init__(self):\n",
    "        self.queue = asyncio.Queue()\n",
    "        self.processor = PipelineProcessor() \n",
    "\n",
    "    async def add_job(self, run_id, user_message, thread_id=\"1\"):\n",
    "        await self.queue.put({\n",
    "            \"run_id\": run_id, \n",
    "            \"message\": user_message, \n",
    "            \"resume\": False\n",
    "        })\n",
    "        print(f\"ğŸ“¥ [ì‹œìŠ¤í…œ] ìš”ì²­ ì ‘ìˆ˜: {run_id}\")\n",
    "\n",
    "    async def add_resume_job(self, run_id, user_message):\n",
    "        await self.queue.put({\n",
    "            \"run_id\": run_id, \n",
    "            \"message\": user_message, \n",
    "            \"resume\": True\n",
    "        })\n",
    "        print(f\"ğŸ“¥ [ì‹œìŠ¤í…œ] ì¬ê°œ ìš”ì²­ ì ‘ìˆ˜: {run_id}\")\n",
    "\n",
    "    async def worker(self):\n",
    "        print(\"ğŸ‘· [ì›Œì»¤] ê°€ë™ ì¤‘... (RAG ì—”ì§„ ì¤€ë¹„ë¨)\")\n",
    "        while True:\n",
    "            job = await self.queue.get()\n",
    "            try:\n",
    "                run_id = job[\"run_id\"]\n",
    "                msg = job[\"message\"]\n",
    "                is_resume = job[\"resume\"]\n",
    "                \n",
    "                print(f\"\\nğŸš€ [ì‹œì‘] {run_id}: {msg}\")\n",
    "                await self.processor.run_pipeline(run_id, msg, resume=is_resume)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ [ì—ëŸ¬] {e}\")\n",
    "            finally:\n",
    "                self.queue.task_done()\n",
    "                print(\"\\n>>> ëª…ë ¹ ì…ë ¥: \", end=\"\") \n",
    "\n",
    "    def trigger_stop(self):\n",
    "        self.processor.stop()\n",
    "\n",
    "# ==========================================\n",
    "# 3. ì¸í„°ë™í‹°ë¸Œ ì‹¤í–‰\n",
    "# ==========================================\n",
    "async def interactive_mode():\n",
    "    # ì—¬ê¸°ì„œ RAG ì—”ì§„ì´ ì´ˆê¸°í™”ë˜ë©° 'chroma_recur_db'ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "    system = AgentQueueSystem()\n",
    "    worker_task = asyncio.create_task(system.worker())\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ğŸ® [RAG íŒŒì´í”„ë¼ì¸] ëª…ë ¹ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”.\")\n",
    "    print(\"1. ìš”ì²­: req <run_id> <ì§ˆë¬¸>  (ì˜ˆ: req run_1 RAGì˜ ì¥ì ì€?)\")\n",
    "    print(\"2. ì¤‘ë‹¨: stop\")\n",
    "    print(\"3. ì¬ê°œ: resume <run_id> <ì§ˆë¬¸>\")\n",
    "    print(\"4. ì¢…ë£Œ: exit\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "    while True:\n",
    "        command = await asyncio.get_event_loop().run_in_executor(None, input, \">>> ëª…ë ¹ ì…ë ¥: \")\n",
    "        parts = command.split()\n",
    "        if not parts: continue\n",
    "        cmd = parts[0].lower()\n",
    "        \n",
    "        if cmd in [\"exit\", \"ì¢…ë£Œ\"]:\n",
    "            if not worker_task.done(): worker_task.cancel()\n",
    "            break\n",
    "        elif cmd == \"stop\":\n",
    "            system.trigger_stop()\n",
    "        elif cmd == \"req\":\n",
    "            if len(parts) < 3:\n",
    "                print(\"âš ï¸ í˜•ì‹ ì˜¤ë¥˜! ì˜ˆ: req run_1 RAGë€?\")\n",
    "                continue\n",
    "            await system.add_job(parts[1], \" \".join(parts[2:]))\n",
    "        elif cmd == \"resume\":\n",
    "            if len(parts) < 3:\n",
    "                print(\"âš ï¸ í˜•ì‹ ì˜¤ë¥˜! ì˜ˆ: resume run_1 RAGë€?\")\n",
    "                continue\n",
    "            await system.add_resume_job(parts[1], \" \".join(parts[2:]))\n",
    "        else:\n",
    "            print(\"â“ ì•Œ ìˆ˜ ì—†ëŠ” ëª…ë ¹ì–´\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await interactive_mode()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
