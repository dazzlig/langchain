import os
import json
from typing import Annotated, List, TypedDict, Dict, Any, Literal
from langchain_core.messages import AIMessage, BaseMessage, SystemMessage
from langgraph.graph import StateGraph, END, START
from langgraph.graph.message import add_messages
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from pydantic import BaseModel, Field
from langchain_community.tools.tavily_search import TavilySearchResults
from langgraph.checkpoint.memory import MemorySaver
from dotenv import load_dotenv

load_dotenv()

# --- Helper Functions ---
def save_step_to_file(run_id, step_name, result):
    directory = f"runs/{run_id}"
    if not os.path.exists(directory):
        os.makedirs(directory)
    
    # Simple file naming strategy for this example
    import time
    timestamp = int(time.time())
    filename = f"{directory}/{timestamp}_{step_name}.json"
    
    data = {
        "step_name": step_name,
        "result": result
    }
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

# --- LLM & Tools ---
llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)
# Ensure Tavily API Key is set in env or handle error
try:
    search_tool = TavilySearchResults(k=3)
except Exception:
    print("Warning: Tavily API Key missing, search will fail if called.")
    search_tool = None # Handle appropriately in node

# ==========================================
# 1. Research Subgraph
# ==========================================
class ResearchState(TypedDict):
    topic: str
    logs: Annotated[List[BaseMessage], add_messages]
    raw_data: str
    quality: str
    retry_count: int
    run_id: str # Added to pass run_id down

def research_execute_node(state: ResearchState):
    print(f"[Research] ì •ë³´ ìˆ˜ì§‘ ì¤‘... Topic: {state['topic']}")
    topic = state["topic"]
    
    try:
        if search_tool:
            results = search_tool.invoke(topic)
            content = "\\n".join([r["content"] for r in results])
        else:
            content = "ê²€ìƒ‰ ë„êµ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (API Key Missing)."
    except Exception as e:
        content = f"ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}"
        
    return {
        "raw_data": content, 
        "logs": [AIMessage(content=f"ê²€ìƒ‰ ì™„ë£Œ: {len(content)}ì", name="researcher")]
    }

def research_reflect_node(state: ResearchState):
    print("[Research Sub] ì •ë³´ ì¶©ë¶„ì„± í‰ê°€ ì¤‘...")
    
    chain = ChatPromptTemplate.from_template(
        """ë‹¹ì‹ ì€ ì—„ê²©í•œ ì—°êµ¬ íŒ€ì¥ì…ë‹ˆë‹¤. ìˆ˜ì§‘ëœ ìë£Œê°€ ì£¼ì œ '{topic}'ì„ ì„¤ëª…í•˜ê¸°ì— ì¶©ë¶„í•œì§€ í‰ê°€í•˜ì„¸ìš”.
        
        [ìˆ˜ì§‘ëœ ìë£Œ]
        {data}
        
        ìë£Œê°€ ì£¼ì œë¥¼ í¬ê´„ì ìœ¼ë¡œ ì„¤ëª…í•˜ë©´ 'PASS', ë¶€ì¡±í•˜ê±°ë‚˜ í¸í–¥ë˜ì—ˆë‹¤ë©´ 'FAIL'ì´ë¼ê³ ë§Œ ë‹µí•˜ì„¸ìš”.
        """
    ) | llm | StrOutputParser()
    
    evaluation = chain.invoke({"topic": state["topic"], "data": state["raw_data"]})
    quality = "PASS" if "PASS" in evaluation else "FAIL"
    
    print(f"      ã„´ í‰ê°€ ê²°ê³¼: {quality}")
    return {"quality": quality, "logs": [AIMessage(content=f"í‰ê°€ ê²°ê³¼: {quality}", name="evaluator")]}

def research_revise_node(state: ResearchState):
    print(" [Research] ì¶”ê°€ ê²€ìƒ‰(ë³´ì™„) ìˆ˜í–‰ ì¤‘...")
    topic = state["topic"]
    current_data = state["raw_data"]
    
    query_chain = ChatPromptTemplate.from_template(
        """ë‹¹ì‹ ì€ ë…¸ë ¨í•œ ë¦¬ì„œì²˜ì…ë‹ˆë‹¤.
        ì£¼ì œ '{topic}'ì— ëŒ€í•´ í˜„ì¬ ìˆ˜ì§‘ëœ ìë£Œê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        
        [í˜„ì¬ ìë£Œ]
        {data}
        
        ìœ„ ìë£Œì—ì„œ ë¹ ì§„ ë‚´ìš©ì´ë‚˜ ë” êµ¬ì²´ì ì¸ ì •ë³´ê°€ í•„ìš”í•œ ë¶€ë¶„ì„ íŒŒì•…í•˜ì—¬,
        ê²€ìƒ‰ ì—”ì§„ì— ì…ë ¥í•  'êµ¬ì²´ì ì¸ ì¶”ê°€ ê²€ìƒ‰ì–´' 1ê°œë¥¼ ì œì•ˆí•´ì£¼ì„¸ìš”. (ì„¤ëª… ì—†ì´ ê²€ìƒ‰ì–´ë§Œ ì¶œë ¥)
        """
    ) | llm | StrOutputParser()
    
    new_query = query_chain.invoke({"topic": topic, "data": current_data[:2000]})
    print(f"      ã„´ìƒì„±ëœ ì¶”ê°€ ê²€ìƒ‰ì–´: '{new_query}'")
    
    try:
        if search_tool:
            search_results = search_tool.invoke(new_query)
            new_content = "\\n".join([f"- {r['content']}" for r in search_results])
        else:
            new_content = "ê²€ìƒ‰ ë„êµ¬ ì—†ìŒ"
    except Exception as e:
        new_content = f"ì¶”ê°€ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}"
        
    combined_data = current_data + f"\\n\\n[ì¶”ê°€ ê²€ìƒ‰ ê²°ê³¼ ({new_query})]:\\n" + new_content
    
    return {
        "raw_data": combined_data, 
        "retry_count": state.get("retry_count", 0) + 1,
        "logs": [AIMessage(content=f"ì¶”ê°€ ê²€ìƒ‰ ì™„ë£Œ: {new_query}", name="researcher")]
    }

def research_submit_node(state: ResearchState):
    summary_chain = ChatPromptTemplate.from_template(
        "ë‹¤ìŒ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ '{topic}'ì— ëŒ€í•œ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½ ì •ë¦¬í•´ì¤˜:\\n\\n{data}"
    ) | llm | StrOutputParser()
    
    final_summary = summary_chain.invoke({"topic": state["topic"], "data": state["raw_data"]})
    
    if "run_id" in state:
        save_step_to_file(state["run_id"], "Research_Done", {"summary": final_summary})
        
    return {"raw_data": final_summary}

research_workflow = StateGraph(ResearchState)
research_workflow.add_node("execute", research_execute_node)
research_workflow.add_node("reflect", research_reflect_node)
research_workflow.add_node("revise", research_revise_node)
research_workflow.add_node("submit", research_submit_node)

research_workflow.add_edge(START, "execute")
research_workflow.add_edge("execute", "reflect")

def route_research(state: ResearchState):
    if state["quality"] == "FAIL" and state.get("retry_count", 0) < 1:
        return "revise"
    return "submit"

research_workflow.add_conditional_edges("reflect", route_research, {"submit": "submit", "revise": "revise"})
research_workflow.add_edge("revise", "submit")
research_workflow.add_edge("submit", END)
research_app = research_workflow.compile()


# ==========================================
# 2. Writer Subgraph
# ==========================================
class WriterState(TypedDict):
    topic: str
    research_data: str
    draft: str
    critique: str
    score: float
    revision_count: int
    logs: Annotated[List[BaseMessage], add_messages]
    code_data: str 
    design_data: str
    run_id: str

def writer_execute_node(state: WriterState):
    count = state.get('revision_count', 0)
    print(f"[Writer Sub] ê¸€ ì‘ì„± ì¤‘... (ë²„ì „ {count + 1})")
    
    chain = ChatPromptTemplate.from_template(
        """ë‹¹ì‹ ì€ ìƒí™©ì— ë§ì¶° ìµœì ì˜ ê¸€ì„ ì“°ëŠ” 'ì „ë¬¸ ìˆ˜ì„ ì—ë””í„°'ì…ë‹ˆë‹¤.
        ì œê³µëœ ì¬ë£Œë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì£¼ì œ '{topic}'ì— ê°€ì¥ ì í•©í•œ í˜•ì‹ì˜ ë¬¸ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.
        
        [ì…ë ¥ ìë£Œ]
        1. ì—°êµ¬ ë‚´ìš©: {data}
        2. ì½”ë“œ ì˜ˆì œ: {code} (ì—†ìœ¼ë©´ 'ì—†ìŒ')
        3. êµ¬ì¡°ë„(Mermaid): {design} (ì—†ìœ¼ë©´ 'ì—†ìŒ')
        4. ì´ì „ ë¹„í‰: {critique}
        
        [ì‘ì„± ì§€ì¹¨]
        1. í˜•ì‹ íŒë‹¨: 
           - ì½”ë“œ/êµ¬ì¡°ë„ê°€ ìˆë‹¤ë©´ 'ê¸°ìˆ  ë¬¸ì„œ'ë‚˜ 'íŠœí† ë¦¬ì–¼' í˜•ì‹ìœ¼ë¡œ, 
           - ì—†ë‹¤ë©´ 'ì—ì„¸ì´', 'ê¸°íšì„œ', 'ë³´ê³ ì„œ' ë“± ì£¼ì œì— ë§ëŠ” í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
           
        2. ìë£Œ í†µí•© (ì¡°ê±´ë¶€ ì‚½ì…):
           - ì—°êµ¬ ë‚´ìš©: ê¸€ì˜ ë…¼ë¦¬ì  ê·¼ê±°ë¡œ í™œìš©í•˜ì„¸ìš”.
           - ì½”ë“œ ì˜ˆì œ: ë‚´ìš©ì´ 'ì—†ìŒ'ì´ ì•„ë‹ˆë¼ë©´, ë°˜ë“œì‹œ ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡(```python ... ```)**ìœ¼ë¡œ ë³¸ë¬¸ì˜ ì ì ˆí•œ ìœ„ì¹˜ì— ì‚½ì…í•˜ì„¸ìš”. (ì–µì§€ë¡œ ë§Œë“¤ì§€ ë§ˆì„¸ìš”)
           - êµ¬ì¡°ë„: ë‚´ìš©ì´ 'ì—†ìŒ'ì´ ì•„ë‹ˆë¼ë©´, ë°˜ë“œì‹œ Mermaid ì½”ë“œ ë¸”ë¡(```mermaid ... ```)**ìœ¼ë¡œ ì‹œê°í™” ì„¹ì…˜ì— ì‚½ì…í•˜ì„¸ìš”.
           
        3. ìŠ¤íƒ€ì¼:
           - ì£¼ì œê°€ í•™ìˆ ì ì´ë©´ ì „ë¬¸ì ìœ¼ë¡œ, ëŒ€ì¤‘ì ì´ë©´ ì½ê¸° ì‰½ê²Œ ì‘ì„±í•˜ì„¸ìš”.
           - ì„œë¡ -ë³¸ë¡ -ê²°ë¡ ì˜ ì™„ê²°ì„± ìˆëŠ” êµ¬ì¡°ë¥¼ ê°–ì¶”ì„¸ìš”.
        """
    ) | llm | StrOutputParser()
    
    draft = chain.invoke({
        "topic": state["topic"],
        "data": state.get("research_data", "ìë£Œ ì—†ìŒ"),
        "code": state.get("code_data", "ì—†ìŒ"), 
        "design": state.get("design_data", "ì—†ìŒ"), 
        "critique": state.get("critique", "ì—†ìŒ")
    })
    
    if "run_id" in state:
        save_step_to_file(state["run_id"], "Write_Done", {"final_draft": draft})
        
    return {
        "draft": draft, 
        "revision_count": count + 1,
        "logs": [AIMessage(content=f"ì´ˆì•ˆ v{count+1} ì‘ì„± ì™„ë£Œ", name="writer")]
    }

def writer_reflect_node(state: WriterState):
    print("[Writer Sub] í’ˆì§ˆ í‰ê°€ ì¤‘...")
    
    chain = ChatPromptTemplate.from_template(
        """ë‹¹ì‹ ì€ ì„¸ê³„ì ì¸ ì €ë„ì˜ 'ì—„ê²©í•œ ìˆ˜ì„ í¸ì§‘ì'ì…ë‹ˆë‹¤. 
        ì•„ë˜ ê¸€ì´ ì‚¬ìš©ì ìš”ì²­ ì£¼ì œì¸ '{topic}'ì— ì™„ë²½í•˜ê²Œ ë¶€í•©í•˜ëŠ”ì§€ ë¹„íŒì ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.
        
        [í‰ê°€ ê¸°ì¤€]
        1. ì£¼ì œ ì í•©ì„±: ìš”ì²­í•œ ì£¼ì œë¥¼ ì •í™•íˆ ë‹¤ë£¨ê³  ìˆëŠ”ê°€?
        2. êµ¬ì²´ì„±: ë§‰ì—°í•œ ë‚´ìš©ì´ ì•„ë‹ˆë¼ êµ¬ì²´ì ì¸ ì‚¬ì‹¤/ì˜ˆì‹œê°€ ìˆëŠ”ê°€?
        3. ë…¼ë¦¬ì  íë¦„: ì„œë¡ -ë³¸ë¡ -ê²°ë¡ ì˜ êµ¬ì¡°ê°€ íƒ„íƒ„í•œê°€?
        
        ì£¼ì˜: ì¡°ê¸ˆì´ë¼ë„ ëª¨í˜¸í•˜ê±°ë‚˜, í‰ë²”í•œ ë‚´ìš©ì´ë¼ë©´ 7ì  ë¯¸ë§Œìœ¼ë¡œ ì ìˆ˜ë¥¼ ì£¼ì„¸ìš”. 
        ì™„ë²½í•˜ì§€ ì•Šìœ¼ë©´ 9ì  ì´ìƒì„ ì£¼ì§€ ë§ˆì„¸ìš”.
        
        í˜•ì‹: ì ìˆ˜/êµ¬ì²´ì ì¸_í”¼ë“œë°± (ì˜ˆ: 6.5/ì£¼ì œì™€ ê´€ë ¨ ì—†ëŠ” ë‚´ìš©ì´ í¬í•¨ë˜ì–´ ìˆê³  ì˜ˆì‹œê°€ ë¶€ì¡±í•©ë‹ˆë‹¤)
        
        [ê¸€]: {draft}
        """
    ) | llm | StrOutputParser()
    
    response = chain.invoke({
        "draft": state["draft"],
        "topic": state["topic"] 
    })
    
    try:
        score_str, fb = response.split("/", 1)
        score = float(score_str.strip().replace("ì ", ""))
    except:
        score, fb = 5.0, "í˜•ì‹ ì˜¤ë¥˜"
        
    print(f"      ã„´ ì ìˆ˜: {score}ì ")
    
    return {
        "score": score, 
        "critique": fb,
        "logs": [AIMessage(content=f"í‰ê°€: {score}ì  / {fb}", name="critic")]
    }
    
writer_workflow = StateGraph(WriterState)
writer_workflow.add_node("execute", writer_execute_node)
writer_workflow.add_node("reflect", writer_reflect_node)
writer_workflow.add_edge(START, "execute")
writer_workflow.add_edge("execute", "reflect")

def route_writer(state: WriterState):
    if state["score"] >= 8.5 or state["revision_count"] >= 3:
        return "end"
    return "execute"

writer_workflow.add_conditional_edges("reflect", route_writer, {"execute": "execute", "end": END})
writer_app = writer_workflow.compile()


# ==========================================
# 3. Code Subgraph
# ==========================================
class CodeState(TypedDict):
    topic: str
    logs: Annotated[List[BaseMessage], add_messages]
    code_result: str
    critique: str
    quality: str
    retry_count: int
    run_id: str

def code_execute_node(state: CodeState):
    print(f"[Code Agent] '{state['topic']}' ì½”ë“œ ì´ˆì•ˆ ì‘ì„± ì¤‘...")
    
    chain = ChatPromptTemplate.from_template(
        """ë‹¹ì‹ ì€ Senior Python ê°œë°œìì…ë‹ˆë‹¤. 
        ì£¼ì œ '{topic}'ì— ëŒ€í•œ Python ì˜ˆì œ ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”.
        
        [ìš”êµ¬ì‚¬í•­]
        1. ì‹¤í–‰ ê°€ëŠ¥í•œ Python ì½”ë“œì—¬ì•¼ í•©ë‹ˆë‹¤.
        2. ì½”ë“œ ë‚´ì— ìƒì„¸í•œ ì£¼ì„(Comments)ì„ í¬í•¨í•˜ì„¸ìš”.
        3. ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡(```python ... ```)ìœ¼ë¡œ ê°ì‹¸ì§€ ë§ê³  ìˆœìˆ˜ ì½”ë“œë§Œ ì¶œë ¥í•˜ê±°ë‚˜, 
           ì½”ë“œ ë¸”ë¡ì„ ì“´ë‹¤ë©´ íŒŒì‹± ê°€ëŠ¥í•œ í˜•íƒœë¡œ ì£¼ì„¸ìš”.
        """
    ) | llm | StrOutputParser()
    
    code = chain.invoke({"topic": state["topic"]})
    
    if "run_id" in state:
        save_step_to_file(state["run_id"], "Code_Done", {"code": code})
        
    return {
        "code_result": code, 
        "retry_count": 0,
        "logs": [AIMessage(content="ì½”ë“œ ì´ˆì•ˆ ìƒì„± ì™„ë£Œ", name="coder")]
    }

def code_reflect_node(state: CodeState):
    print("[Code Agent] ì½”ë“œ í’ˆì§ˆ ë¦¬ë·° ì¤‘...")
    
    chain = ChatPromptTemplate.from_template(
        """ë‹¹ì‹ ì€ ê¹Œë‹¤ë¡œìš´ ì½”ë“œ ë¦¬ë·°ì–´(Code Reviewer)ì…ë‹ˆë‹¤.
        ì•„ë˜ ì½”ë“œë¥¼ ê²€í† í•˜ê³  ì ìˆ˜ì™€ í”¼ë“œë°±ì„ ì œê³µí•˜ì„¸ìš”.
        
        [ê²€í† í•  ì½”ë“œ]
        {code}
        
        [í‰ê°€ ê¸°ì¤€]
        1. ë¬¸ë²• ì˜¤ë¥˜(Syntax Error)ê°€ ì—†ëŠ”ê°€?
        2. ì£¼ì„(Comments)ì´ ì¶©ë¶„íˆ ì‘ì„±ë˜ì—ˆëŠ”ê°€?
        3. ì‹¤í–‰ ê°€ëŠ¥í•œ êµ¬ì¡°ì¸ê°€?
        
        [ì¶œë ¥ í˜•ì‹]
        ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”:
        ìƒíƒœ: [PASS ë˜ëŠ” FAIL]
        í”¼ë“œë°±: [êµ¬ì²´ì ì¸ ê°œì„ ì  ë˜ëŠ” ì˜¤ë¥˜ ë‚´ìš©]
        """
    ) | llm | StrOutputParser()
    
    review_result = chain.invoke({"code": state["code_result"]})
    
    try:
        status_line = review_result.split("\\n")[0]
        quality = "PASS" if "PASS" in status_line else "FAIL"
        critique = review_result
    except:
        quality = "FAIL"
        critique = "ë¦¬ë·° í˜•ì‹ ì˜¤ë¥˜ ë°œìƒ"

    print(f"      ã„´ ë¦¬ë·° ê²°ê³¼: {quality}")
    return {
        "quality": quality, 
        "critique": critique,
        "logs": [AIMessage(content=f"ë¦¬ë·° ì™„ë£Œ: {quality}", name="reviewer")]
    }

def code_revise_node(state: CodeState):
    print(" [Code Agent] í”¼ë“œë°± ë°˜ì˜í•˜ì—¬ ì½”ë“œ ìˆ˜ì • ì¤‘...")
    
    chain = ChatPromptTemplate.from_template(
        """ë‹¹ì‹ ì€ ê°œë°œìì…ë‹ˆë‹¤. ë¦¬ë·°ì–´ì˜ í”¼ë“œë°±ì„ ë°˜ì˜í•˜ì—¬ ì½”ë“œë¥¼ ìˆ˜ì •í•˜ì„¸ìš”.
        
        [ê¸°ì¡´ ì½”ë“œ]
        {code}
        
        [ë¦¬ë·°ì–´ í”¼ë“œë°±]
        {critique}
        
        í”¼ë“œë°±ì„ ë°˜ì˜í•˜ì—¬ ê°œì„ ëœ 'ì „ì²´ ì½”ë“œ'ë§Œ ë‹¤ì‹œ ì¶œë ¥í•˜ì„¸ìš”. (ì„¤ëª… ì œì™¸)
        """
    ) | llm | StrOutputParser()
    
    new_code = chain.invoke({
        "code": state["code_result"],
        "critique": state["critique"]
    })
    
    return {
        "code_result": new_code,
        "retry_count": state["retry_count"] + 1,
        "logs": [AIMessage(content=f"ì½”ë“œ ìˆ˜ì • ì™„ë£Œ (ì‹œë„ {state['retry_count']+1}íšŒ)", name="coder")]
    }

code_workflow = StateGraph(CodeState)
code_workflow.add_node("execute", code_execute_node)
code_workflow.add_node("reflect", code_reflect_node)
code_workflow.add_node("revise", code_revise_node)

code_workflow.add_edge(START, "execute")
code_workflow.add_edge("execute", "reflect")

def route_code(state: CodeState):
    if state["quality"] == "PASS" or state["retry_count"] >= 3:
        return END
    return "revise"

code_workflow.add_conditional_edges("reflect", route_code, {"revise": "revise", END: END})
code_workflow.add_edge("revise", "reflect")
code_app = code_workflow.compile()


# ==========================================
# 4. Designer Subgraph
# ==========================================
class DesignerState(TypedDict):
    topic: str
    logs: Annotated[List[BaseMessage], add_messages]
    design_result: str
    critique: str
    quality: str
    retry_count: int
    run_id: str

def designer_execute_node(state: DesignerState):
    print(f"[Designer Agent] '{state['topic']}' ì‹œê°í™” êµ¬ì¡° ì„¤ê³„ ì¤‘...")
    
    chain = ChatPromptTemplate.from_template(
        """ë‹¹ì‹ ì€ ì‹œìŠ¤í…œ ì•„í‚¤í…íŠ¸ì…ë‹ˆë‹¤. 
        ì£¼ì œ '{topic}'ì˜ êµ¬ì¡°ë‚˜ íë¦„ì„ ê°€ì¥ ì˜ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” 'Mermaid ë‹¤ì´ì–´ê·¸ë¨' ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”.
        
        [ìš”êµ¬ì‚¬í•­]
        1. íë¦„ë„(graph TD) ë˜ëŠ” ì‹œí€€ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨(sequenceDiagram) ì¤‘ ì ì ˆí•œ ê²ƒì„ ì„ íƒí•˜ì„¸ìš”.
        2. ì„¤ëª… í…ìŠ¤íŠ¸ ì—†ì´ ì˜¤ì§ Mermaid ì½”ë“œë§Œ ì¶œë ¥í•˜ì„¸ìš”.
        3. ë§ˆí¬ë‹¤ìš´ íƒœê·¸(```mermaid)ëŠ” ì œì™¸í•˜ê³  ìˆœìˆ˜ ì½”ë“œë§Œ ì£¼ì„¸ìš”.
        """
    ) | llm | StrOutputParser()
    
    design = chain.invoke({"topic": state["topic"]})
    
    if "run_id" in state:
        save_step_to_file(state["run_id"], "Design_Done", {"design": design})
        
    return {
        "design_result": design,
        "retry_count": 0,
        "logs": [AIMessage(content="ë‹¤ì´ì–´ê·¸ë¨ ì´ˆì•ˆ ìƒì„± ì™„ë£Œ", name="designer")]
    }

def designer_reflect_node(state: DesignerState):
    print("[Designer Agent] ë‹¤ì´ì–´ê·¸ë¨ ë¬¸ë²• ë° ì ì ˆì„± ê²€ì‚¬ ì¤‘...")
    
    chain = ChatPromptTemplate.from_template(
        """ë‹¹ì‹ ì€ Mermaid ë¬¸ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
        ì•„ë˜ ì½”ë“œê°€ ë¬¸ë²•ì ìœ¼ë¡œ ì˜¬ë°”ë¥´ê³  ì£¼ì œë¥¼ ì˜ í‘œí˜„í•˜ëŠ”ì§€ ê²€ì‚¬í•˜ì„¸ìš”.
        
        [ê²€í† í•  ì½”ë“œ]
        {code}
        
        [ì¶œë ¥ í˜•ì‹]
        ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”:
        ìƒíƒœ: [PASS ë˜ëŠ” FAIL]
        í”¼ë“œë°±: [ì˜¤ë¥˜ ë‚´ìš© ë˜ëŠ” ê°œì„ ì ]
        """
    ) | llm | StrOutputParser()
    
    review_result = chain.invoke({"code": state["design_result"]})
    
    try:
        status_line = review_result.split("\\n")[0]
        quality = "PASS" if "PASS" in status_line else "FAIL"
        critique = review_result
    except:
        quality = "FAIL"
        critique = "í˜•ì‹ ì˜¤ë¥˜ ë°œìƒ"
        
    print(f"      ã„´ ê²€ì‚¬ ê²°ê³¼: {quality}")
    return {
        "quality": quality,
        "critique": critique,
        "logs": [AIMessage(content=f"ê²€ì‚¬ ì™„ë£Œ: {quality}", name="reviewer")]
    }

def designer_revise_node(state: DesignerState):
    print("[Designer Agent] í”¼ë“œë°± ë°˜ì˜í•˜ì—¬ ìˆ˜ì • ì¤‘...")
    
    chain = ChatPromptTemplate.from_template(
        """ë‹¹ì‹ ì€ ë””ìì´ë„ˆì…ë‹ˆë‹¤. í”¼ë“œë°±ì„ ë°˜ì˜í•˜ì—¬ Mermaid ì½”ë“œë¥¼ ìˆ˜ì •í•˜ì„¸ìš”.
        
        [ê¸°ì¡´ ì½”ë“œ]
        {code}
        
        [í”¼ë“œë°±]
        {critique}
        
        ìˆ˜ì •ëœ ì „ì²´ Mermaid ì½”ë“œë§Œ ì¶œë ¥í•˜ì„¸ìš”. (ì„¤ëª… ì œì™¸)
        """
    ) | llm | StrOutputParser()
    
    new_design = chain.invoke({
        "code": state["design_result"],
        "critique": state["critique"]
    })
    
    return {
        "design_result": new_design,
        "retry_count": state["retry_count"] + 1,
        "logs": [AIMessage(content=f"ìˆ˜ì • ì™„ë£Œ (ì‹œë„ {state['retry_count']+1}íšŒ)", name="designer")]
    }

designer_workflow = StateGraph(DesignerState)
designer_workflow.add_node("execute", designer_execute_node)
designer_workflow.add_node("reflect", designer_reflect_node)
designer_workflow.add_node("revise", designer_revise_node)

designer_workflow.add_edge(START, "execute")
designer_workflow.add_edge("execute", "reflect")

def route_design(state: DesignerState):
    if state["quality"] == "PASS" or state["retry_count"] >= 3:
        return END
    return "revise"

designer_workflow.add_conditional_edges("reflect", route_design, {"revise": "revise", END: END})
designer_workflow.add_edge("revise", "reflect")
designer_app = designer_workflow.compile()


# ==========================================
# 5. Main Supervisor Graph
# ==========================================
def update_agent_results(existing: Dict[str, Any], new_data: Dict[str, Any]) -> Dict[str, Any]:
    if existing is None:
        return new_data
    merged = existing.copy()
    merged.update(new_data)
    return merged

class MainState(TypedDict):
    messages: Annotated[List[BaseMessage], add_messages]
    agent_results: Annotated[Dict[str, Any], update_agent_results]
    next: List[str]
    run_id: str # Pass run_id

class SupervisorDecision(BaseModel):
    next: List[Literal['research_subgraph', 'code_subgraph', 'designer_subgraph', 'writer_subgraph', 'FINISH']] = Field(
        description="ë‹¤ìŒì— ì‹¤í–‰í•  ì—ì´ì „íŠ¸ ëª©ë¡. ë³‘ë ¬ ì‹¤í–‰ì´ í•„ìš”í•˜ë©´ ì—¬ëŸ¬ ê°œë¥¼ ì„ íƒí•˜ì„¸ìš”."
    )
    reasoning: str = Field(description="ì´ ê²°ì •ì„ ë‚´ë¦° ì´ìœ  (ì„±ì°°)")

def supervisor_node(state: MainState):
    results = state.get("agent_results", {})
    messages = state.get("messages", [])
    last_user_msg = messages[-1].content if messages else ""
    
    status = {
        "research": "ìˆìŒ" if "research" in results else "ì—†ìŒ",
        "code": "ìˆìŒ" if "code" in results else "ì—†ìŒ",
        "design": "ìˆìŒ" if "design" in results else "ì—†ìŒ",
        "final_doc": "ìˆìŒ" if "final_doc" in results else "ì—†ìŒ"
    }
    
    system_prompt = f"""ë‹¹ì‹ ì€ ìœ ëŠ¥í•œ AI í”„ë¡œì íŠ¸ ë§¤ë‹ˆì €ì…ë‹ˆë‹¤. 
    ì‚¬ìš©ìì˜ ìš”ì²­ê³¼ í˜„ì¬ ì‘ì—… ìƒíƒœë¥¼ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ì‘ì—…ì(ë“¤)ì„ ì§€ì •í•˜ì„¸ìš”.
    
    [ì‚¬ìš©ì ìš”ì²­]: "{last_user_msg}"
    
    [í˜„ì¬ ë°ì´í„° ìƒíƒœ]
    {status}
    
    [íŒë‹¨ ê°€ì´ë“œ]
    1. ì½”ë“œ/ë””ìì¸ í•„ìš”ì„± íŒë‹¨:
           - ìš”ì²­ì´ 'êµ¬í˜„', 'ê°œë°œ', 'ì„¤ê³„', 'ì•Œê³ ë¦¬ì¦˜', 'êµ¬ì¡°ë„' ë“±ì„ í¬í•¨í•˜ë‚˜ìš”? -> Code/Designer í˜¸ì¶œ
           - ë‹¨ìˆœ 'ë™í–¥ íŒŒì•…', 'ë¶„ì„ ë³´ê³ ì„œ', 'ì—ì„¸ì´'ì¸ê°€ìš”? -> Researchë§Œ í˜¸ì¶œ (Code/Design ìƒëµ)
    
    2. ì‘ì—… ìˆœì„œ:
           1. **0ìˆœìœ„: ë¬´ì¡°ê±´ ì¢…ë£Œ**:
           - 'Final Document' ìƒíƒœê°€ 'ìˆìŒ'ì´ë¼ë©´, ë‹¤ë¥¸ ì¡°ê±´ ë³¼ ê²ƒ ì—†ì´ ë¬´ì¡°ê±´ 'FINISH'ë¥¼ ì„ íƒí•˜ì„¸ìš”. (ì ˆëŒ€ Writerë¥¼ ë‹¤ì‹œ ë¶€ë¥´ì§€ ë§ˆì„¸ìš”)
    
    2. **ì¤‘ë³µ ì‹¤í–‰ ê¸ˆì§€**: 
           - ì´ë¯¸ 'Code ê²°ê³¼'ê°€ 'ìˆìŒ'ì´ë¼ë©´, ì ˆëŒ€ë¡œ 'code_subgraph'ë¥¼ ë‹¤ì‹œ í˜¸ì¶œí•˜ì§€ ë§ˆì„¸ìš”.
           - ì´ë¯¸ 'Research ê²°ê³¼'ê°€ 'ìˆìŒ'ì´ë¼ë©´, ì ˆëŒ€ë¡œ 'research_subgraph'ë¥¼ ë‹¤ì‹œ í˜¸ì¶œí•˜ì§€ ë§ˆì„¸ìš”.
    
    3. **ì‘ì—… íë¦„**:
           - (1ë‹¨ê³„) ìë£Œ ìƒì„±: ìš”ì²­ì— ë”°ë¼ Research, Code, Design íŒ€ì„ í˜¸ì¶œí•©ë‹ˆë‹¤.
           - (2ë‹¨ê³„) ë¬¸ì„œ ì‘ì„±: ìœ„ ìë£Œë“¤ì´ ì¤€ë¹„ë˜ì—ˆê³ , ì•„ì§ 'final_doc'ê°€ 'ì—†ìŒ'ì´ë¼ë©´ -> 'writer_subgraph'ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”.
       
    4. **íŠ¹ìˆ˜ ìƒí™©**:
           - ë§Œì•½ ì‚¬ìš©ìê°€ ì½”ë“œë§Œ ìš”ì²­í–ˆê³  'Code ê²°ê³¼'ëŠ” ìˆëŠ”ë° 'Final Document'ê°€ ì—†ë‹¤ë©´ -> 'writer_subgraph'ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”.
    3. ë³‘ë ¬ ì‹¤í–‰:
           - ì—°êµ¬, ì½”ë“œ, ë””ìì¸ì´ ëª¨ë‘ í•„ìš”í•˜ë‹¤ê³  íŒë‹¨ë˜ë©´ ë™ì‹œì— í˜¸ì¶œí•˜ì„¸ìš”.
    """
    
    
    print(f"\\n[Main Supervisor] í˜„ì¬ ìƒíƒœ: {status}")

    model = llm.with_structured_output(SupervisorDecision)
    decision = model.invoke([SystemMessage(content=system_prompt)])
    
    # ğŸ›‘ Safeguard: If Research is done but LLM selects Research again -> Redirect to Writer
    if "research_subgraph" in decision.next and status["research"] == "ìˆìŒ":
        print("âš ï¸ [Override] Research already done. Switching to Writer.")
        decision.next = ["writer_subgraph"]

    print(f"\\n[Main Supervisor] ì§€ì‹œ: {decision.next}")
    return {"next": decision.next}

def call_research_subgraph(state: MainState):
    print("[Main] 'Research ì„œë¸Œê·¸ë˜í”„' í˜¸ì¶œ")
    topic = state["messages"][0].content
    output = research_app.invoke({"topic": topic, "run_id": state.get("run_id","")})
    return {"agent_results": {"research": output["raw_data"]}}

def call_writer_subgraph(state: MainState):
    print("\\n[Main] 'Writer ì„œë¸Œê·¸ë˜í”„' í˜¸ì¶œ")
    topic = state["messages"][0].content
    results = state["agent_results"]
    
    output = writer_app.invoke({
        "topic": topic, 
        "research_data": results.get("research", ""),
        "code_data": results.get("code", ""),
        "design_data": results.get("design", ""),
        "revision_count": 0,
        "run_id": state.get("run_id","")
    })
    
    return {"agent_results": {"final_doc": output["draft"]}}

def call_code_subgraph(state: MainState):
    print("[Main] 'Code íŒ€' (ì„œë¸Œê·¸ë˜í”„) í˜¸ì¶œ")
    topic = state["messages"][0].content
    output = code_app.invoke({
        "topic": topic,
        "retry_count": 0,
        "run_id": state.get("run_id","")
    })
    return {"agent_results": {"code": output["code_result"]}}

def call_designer_subgraph(state: MainState):
    print("[Main] 'Designer íŒ€' (ì„œë¸Œê·¸ë˜í”„) í˜¸ì¶œ")
    topic = state["messages"][0].content
    output = designer_app.invoke({
        "topic": topic,
        "retry_count": 0,
        "run_id": state.get("run_id","")
    })
    return {"agent_results": {"design": output["design_result"]}}

main_workflow = StateGraph(MainState)
main_workflow.add_node("supervisor", supervisor_node)
main_workflow.add_node("research_subgraph", call_research_subgraph)
main_workflow.add_node("writer_subgraph", call_writer_subgraph)
main_workflow.add_node("code_subgraph", call_code_subgraph)
main_workflow.add_node("designer_subgraph", call_designer_subgraph)

main_workflow.add_edge(START, "supervisor")
main_workflow.add_edge("research_subgraph", "supervisor")
main_workflow.add_edge("writer_subgraph", "supervisor")
main_workflow.add_edge("code_subgraph", "supervisor")
main_workflow.add_edge("designer_subgraph", "supervisor")

def route_supervisor(state: MainState):
    next_agents = state["next"]
    if "FINISH" in next_agents:
        return END
    return next_agents

main_workflow.add_conditional_edges(
    "supervisor",
    route_supervisor,
    {
        "research_subgraph": "research_subgraph",
        "code_subgraph": "code_subgraph",
        "designer_subgraph": "designer_subgraph",
        "writer_subgraph": "writer_subgraph",
        END: END
    }
)

memory = MemorySaver()
app = main_workflow.compile(checkpointer=memory)
